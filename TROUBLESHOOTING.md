# GPT-2 ä¸­æ–‡é¢„è®­ç»ƒè¸©å‘æŒ‡å—

> æœ¬æ–‡æ¡£è®°å½•äº† A100/5090 è®­ç»ƒè„šæœ¬å¼€å‘è¿‡ç¨‹ä¸­é‡åˆ°çš„æ‰€æœ‰é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆï¼Œé¿å…åç»­è¸©å‘ã€‚

---

## ğŸ”´ ä¸¥é‡é—®é¢˜ (ä¼šå¯¼è‡´å´©æºƒ)

### 1. DDP + torch.compile + Gradient Checkpointing å†²çª

**æŠ¥é”™ä¿¡æ¯**:
```
RuntimeError: Parameter ... has been marked as ready twice
RuntimeError: expect_autograd_hooks_ INTERNAL ASSERT FAILED
```

**åŸå› **: ä¸‰è€…åŒæ—¶å¯ç”¨ä¼šå¯¼è‡´ PyTorch å†…éƒ¨çŠ¶æ€å†²çª

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ–¹æ¡ˆ1: äºŒé€‰ä¸€ (æ¨è torch.compileï¼ŒA100 40GB å¤Ÿç”¨)
gradient_checkpointing=not args.use_compile,
torch_compile=args.use_compile,

# æ–¹æ¡ˆ2: ç¦ç”¨ torch.compile
gradient_checkpointing=True,
torch_compile=False,
```

---

### 2. æ‰‹åŠ¨ DDP + Trainer è‡ªåŠ¨ DDP = åŒé‡åŒ…è£…

**æŠ¥é”™ä¿¡æ¯**:
```
RuntimeError: Expected all tensors to be on the same device
AttributeError: 'DistributedDataParallel' object has no attribute 'config'
```

**åŸå› **: åŒæ—¶æ‰‹åŠ¨è°ƒç”¨ `DDP(model)` å’Œè®¾ç½® `TrainingArguments(local_rank=...)`

**è§£å†³æ–¹æ¡ˆ**:
```python
# âŒ é”™è¯¯åšæ³•
model = DDP(model, device_ids=[local_rank])
trainer = Trainer(model=model, ...)

# âœ… æ­£ç¡®åšæ³• - è®© Trainer è‡ªåŠ¨å¤„ç†
trainer = Trainer(model=model, ...)  # ç›´æ¥ä¼ å…¥ï¼Œä¸è¦æ‰‹åŠ¨ DDP
```

---

### 3. torch.compile ä¸ Checkpoint ä¿å­˜å†²çª

**æŠ¥é”™ä¿¡æ¯**:
```
KeyError: '_orig_mod.transformer.wte.weight'
RuntimeError: Error(s) in loading state_dict
```

**åŸå› **: `torch.compile` ä¼šå°†æƒé‡ååŠ ä¸Š `_orig_mod` å‰ç¼€

**è§£å†³æ–¹æ¡ˆ**:
```python
# TrainingArguments è‡ªåŠ¨å¤„ç†
training_args = TrainingArguments(
    torch_compile=True,  # è®© Trainer ç®¡ç† compile
    ...
)
```

---

### 4. bitsandbytes CUDA 12 å…¼å®¹æ€§

**æŠ¥é”™ä¿¡æ¯**:
```
The installed version of bitsandbytes was compiled without GPU support
libbitsandbytes_cuda124.so: cannot open shared object file
```

**åŸå› **: bitsandbytes äºŒè¿›åˆ¶ç‰ˆæœ¬ä¸ CUDA 12.x ä¸å®Œå…¨å…¼å®¹

**è§£å†³æ–¹æ¡ˆ**:
```python
def get_optimizer_name(use_8bit_adam):
    if not use_8bit_adam:
        return "adamw_torch_fused"
    try:
        import bitsandbytes as bnb
        test_param = torch.zeros(1, device='cuda', requires_grad=True)
        bnb.optim.Adam8bit([test_param])  # æµ‹è¯•æ˜¯å¦çœŸæ­£æ”¯æŒ
        return "adamw_bnb_8bit"
    except Exception:
        return "adamw_torch_fused"  # Fallback
```

---

### 5. resume æ—  Checkpoint æ—¶å´©æºƒ

**æŠ¥é”™ä¿¡æ¯**:
```
ValueError: No valid checkpoint found in output directory
```

**è§£å†³æ–¹æ¡ˆ**:
```python
resume_path = None
if args.resume:
    checkpoint_dir = Path(args.work_dir) / "checkpoints"
    checkpoints = list(checkpoint_dir.glob("checkpoint-*"))
    if checkpoints:
        resume_path = True
    else:
        print("âš ï¸ æœªæ‰¾åˆ° checkpointï¼Œä»å¤´å¼€å§‹")
trainer.train(resume_from_checkpoint=resume_path)
```

---

## ğŸŸ  ä¸­ç­‰é—®é¢˜ (æ€§èƒ½ä¸‹é™)

### 6. DataLoader num_workers è¿‡ä½

**ç°è±¡**: GPU åˆ©ç”¨ç‡ 50-70%ï¼Œ`nvidia-smi` æ˜¾ç¤º GPU ç»å¸¸ç©ºé—²

**è§£å†³æ–¹æ¡ˆ**:
```python
dataloader_num_workers=min(8, multiprocessing.cpu_count()),
dataloader_persistent_workers=True,  # é¿å…æ¯ epoch é‡å»º
dataloader_pin_memory=True,
```

---

### 7. check_environment è¿”å› None

**é—®é¢˜**: DDP æ—¶éä¸»è¿›ç¨‹ Flash Attention å¤±æ•ˆ

```python
# âŒ é”™è¯¯
def check_environment(local_rank=0):
    if local_rank != 0:
        return  # è¿”å› None

# âœ… æ­£ç¡®
def check_environment(local_rank=0):
    if local_rank != 0:
        return False  # æ˜ç¡®è¿”å› False
```

---

### 8. Hub ä»“åº“ä¸å­˜åœ¨æ—¶ä¸Šä¼ å¤±è´¥

**æŠ¥é”™ä¿¡æ¯**:
```
huggingface_hub.utils._errors.RepositoryNotFoundError
```

**è§£å†³æ–¹æ¡ˆ**:
```python
api.create_repo(repo_id=repo_id, exist_ok=True, private=False)  # å…ˆåˆ›å»º
api.upload_folder(folder_path=..., repo_id=repo_id, ...)
```

---

### 9. SentencePiece è®­ç»ƒæ…¢

**ç°è±¡**: åˆ†è¯å™¨è®­ç»ƒ 10+ åˆ†é’Ÿ

**è§£å†³æ–¹æ¡ˆ**:
```python
spm.SentencePieceTrainer.train(
    input_sentence_size=200000,  # é™åˆ¶æ ·æœ¬æ•°
    shuffle_input_sentence=True,
    num_threads=multiprocessing.cpu_count(),
    ...
)
```

---

### 10. map batch_size è¿‡å¤§å¯¼è‡´ OOM

**æŠ¥é”™ä¿¡æ¯**:
```
Killed (signal 9)
MemoryError
```

**è§£å†³æ–¹æ¡ˆ**:
```python
dataset.map(
    tokenize_fn,
    batch_size=2000,  # ä» 5000 é™åˆ° 2000
    num_proc=min(8, cpu_count()),
)
```

---

## ğŸŸ¡ æ³¨æ„äº‹é¡¹

### 11. Flash Attention éœ€è¦ Ampere+ æ¶æ„

```python
if torch.cuda.get_device_capability()[0] >= 8:  # SM 8.0+
    # æ”¯æŒ Flash Attention (A100, A10, RTX 30xx, 40xx, 50xx)
else:
    # ä¸æ”¯æŒ (V100, T4, P100)
```

---

### 12. TF32 åªæœ‰ Ampere+ æ”¯æŒ

```python
# A100/A800/H100/5090 å¯ç”¨ï¼ŒV100/T4 ä¸å¯ç”¨
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
```

---

### 13. ç¯å¢ƒå˜é‡è®¾ç½®é¡ºåº

```python
# å¿…é¡»åœ¨ä»»ä½• HuggingFace å¯¼å…¥ä¹‹å‰è®¾ç½®
os.environ.setdefault("HF_HOME", "/root/autodl-tmp/cache")
os.environ.setdefault("TRANSFORMERS_CACHE", "/root/autodl-tmp/cache")

# ç„¶åæ‰èƒ½ import
from transformers import ...
```

---

### 14. å®Œæ•´éšæœºç§å­è®¾ç½®

```python
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
```

---

## ğŸ“Š GPU æ¨èé…ç½®

| GPU | batch_size | compile_mode | ç‰¹æ®Šä¼˜åŒ– |
|-----|-----------|--------------|---------|
| A100-40GB | 48 | reduce-overhead | TF32 |
| A100-80GB | 64 | reduce-overhead | TF32 |
| RTX 5090 | 48 | max-autotune | - |
| RTX 4090 | 32 | max-autotune | - |
| T4 (Kaggle) | 8 | ä¸æ¨è | DataParallel |

---

## ğŸ”§ è°ƒè¯•å‘½ä»¤

```bash
# æŸ¥çœ‹ GPU çŠ¶æ€
watch -n 1 nvidia-smi

# æŸ¥çœ‹ CUDA ç‰ˆæœ¬
nvcc --version
python -c "import torch; print(torch.version.cuda)"

# æµ‹è¯• bitsandbytes
python -c "import bitsandbytes; print(bitsandbytes.__version__)"

# æµ‹è¯• Flash Attention
python -c "import flash_attn; print(flash_attn.__version__)"

# æŸ¥çœ‹ GPU æ¶æ„
python -c "import torch; print(torch.cuda.get_device_capability())"
```

---

## ğŸ“ ç‰ˆæœ¬è¦æ±‚

| ä¾èµ– | æœ€ä½ç‰ˆæœ¬ | æ¨èç‰ˆæœ¬ |
|-----|---------|---------|
| Python | 3.10 | 3.11 |
| PyTorch | 2.0 | 2.2+ |
| CUDA | 11.8 (A100) | 12.1+ |
| Transformers | 4.36 | 4.40+ |
| flash-attn | 2.0 | 2.5+ |

---

*æœ€åæ›´æ–°: 2026-02-03*
