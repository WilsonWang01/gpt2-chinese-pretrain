#!/usr/bin/env python3
"""
GPT-2 Chinese Pretraining Script for A100/A800 (v2 - FIXED)
============================================================
é’ˆå¯¹ NVIDIA A100/A800 (Ampere æ¶æ„) æ·±åº¦ä¼˜åŒ–
å·²ä¿®å¤æ‰€æœ‰å·²çŸ¥é—®é¢˜ï¼Œç”Ÿäº§ç¯å¢ƒå°±ç»ª

ä¿®å¤è®°å½•:
- [FIX-1] ç§»é™¤ DDP+compile+gradient_checkpointing å†²çª
- [FIX-2] ä½¿ç”¨ Trainer å†…ç½® DDPï¼Œé¿å… model.module é—®é¢˜
- [FIX-3] ç§»é™¤æ‰‹åŠ¨ DDP åŒ…è£…
- [FIX-4] bitsandbytes fallback é€»è¾‘
- [FIX-5] å¢åŠ  dataloader_num_workers
- [FIX-6] å¯ç”¨ dataloader_persistent_workers
- [FIX-7] SentencePiece è®­ç»ƒåŠ é€Ÿ
- [FIX-8] æ•°æ®é›†å¹¶è¡Œä¸‹è½½
- [FIX-9] é™ä½ map batch_size
- [FIX-10-15] å…¶ä»–ä¼˜åŒ–

è¿è¡Œæ–¹å¼ï¼ˆå•å¡ï¼‰ï¼š
    python a100_train.py

è¿è¡Œæ–¹å¼ï¼ˆå¤šå¡ DDPï¼‰ï¼š
    torchrun --nproc_per_node=2 a100_train.py

ç¯å¢ƒå®‰è£…ï¼š
    pip install torch transformers datasets accelerate sentencepiece tokenizers bitsandbytes liger-kernel huggingface_hub
    pip install flash-attn --no-build-isolation  # å¯é€‰
"""

import os
import sys
import time
import gc
import json
import random
import argparse
import multiprocessing
from pathlib import Path
from itertools import chain
from concurrent.futures import ThreadPoolExecutor

import numpy as np
import torch
import torch.distributed as dist

# [FIX-15] è®¾ç½® HF ç¼“å­˜ç¯å¢ƒå˜é‡ï¼ˆåœ¨ä»»ä½• HuggingFace å¯¼å…¥ä¹‹å‰ï¼‰
os.environ.setdefault("HF_HOME", "/root/autodl-tmp/cache")
os.environ.setdefault("TRANSFORMERS_CACHE", "/root/autodl-tmp/cache")


# ============================================================
# A100 ä¸“å±ä¼˜åŒ–
# ============================================================

def enable_a100_optimizations(local_rank=0):
    """å¯ç”¨ A100 ä¸“å±ä¼˜åŒ– [FIX-10: åªåœ¨ä¸»è¿›ç¨‹æ‰“å°]"""
    # === 1. TF32 ä¼˜åŒ– (A100 ç‹¬æœ‰) ===
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    
    # === 2. cuDNN è‡ªåŠ¨è°ƒä¼˜ ===
    torch.backends.cudnn.benchmark = True
    
    # === 3. å†…å­˜åˆ†é…ä¼˜åŒ– ===
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
    
    if local_rank == 0:
        print("=" * 60)
        print("âš¡ A100/Ampere ä¸“å±ä¼˜åŒ–")
        print("=" * 60)
        print("   âœ… TF32: å·²å¯ç”¨ (çŸ©é˜µä¹˜æ³•åŠ é€Ÿ 2-6x)")
        print("   âœ… cuDNN Autotuner: å·²å¯ç”¨")
        print("   âœ… CUDA å†…å­˜åˆ†é…: å·²ä¼˜åŒ–")
        print("=" * 60)


# ============================================================
# ç¯å¢ƒæ£€æŸ¥
# ============================================================

def check_environment(local_rank=0):
    """æ£€æŸ¥ A100 ç¯å¢ƒå…¼å®¹æ€§"""
    if local_rank != 0:
        # [FIX] éä¸»è¿›ç¨‹è¿”å› False è€Œé Noneï¼Œç¡®ä¿ Flash Attention åœ¨ DDP æ—¶æ­£ç¡®å¯ç”¨
        return False
    
    print("=" * 60)
    print("ğŸ” A100 ç¯å¢ƒæ£€æŸ¥")
    print("=" * 60)
    
    # Python ç‰ˆæœ¬
    py_version = sys.version_info
    print(f"   Python: {py_version.major}.{py_version.minor}.{py_version.micro}", end="")
    if py_version.major >= 3 and py_version.minor >= 10:
        print(" âœ…")
    else:
        print(" âš ï¸ æ¨è 3.10+")
    
    # PyTorch ç‰ˆæœ¬
    print(f"   PyTorch: {torch.__version__}", end="")
    torch_major = int(torch.__version__.split('.')[0])
    if torch_major >= 2:
        print(" âœ…")
    else:
        print(" âš ï¸ æ¨è 2.0+")
    
    # CUDA
    if torch.cuda.is_available():
        cuda_version = torch.version.cuda
        print(f"   CUDA: {cuda_version}", end="")
        cuda_major = float(cuda_version.split('.')[0]) if cuda_version else 0
        if cuda_major >= 11:
            print(" âœ…")
        else:
            print(" âš ï¸ æ¨è CUDA 11.8+")
        
        # GPU ä¿¡æ¯
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_mem = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"   GPU {i}: {gpu_name} ({gpu_mem:.0f}GB)")
            
            if "A100" in gpu_name or "A800" in gpu_name:
                if gpu_mem >= 75:
                    print(f"         â†’ 80GB ç‰ˆæœ¬ (æ¨è batch_size=64+)")
                else:
                    print(f"         â†’ 40GB ç‰ˆæœ¬ (æ¨è batch_size=48)")
    else:
        print("   CUDA: âŒ æœªæ£€æµ‹åˆ° GPU!")
        sys.exit(1)
    
    # Transformers
    try:
        import transformers
        print(f"   Transformers: {transformers.__version__} âœ…")
    except ImportError:
        print("   Transformers: âŒ æœªå®‰è£…!")
        sys.exit(1)
    
    # [FIX-4] bitsandbytes æ£€æµ‹
    try:
        import bitsandbytes as bnb
        # æµ‹è¯•æ˜¯å¦çœŸæ­£æ”¯æŒ GPU
        bnb.optim.Adam8bit([torch.zeros(1, device='cuda', requires_grad=True)])
        print(f"   bitsandbytes: âœ… GPU æ”¯æŒ")
    except Exception as e:
        print(f"   bitsandbytes: âš ï¸ æ—  GPU æ”¯æŒ (å°†ä½¿ç”¨ fused AdamW)")
    
    # [FIX-13] Flash Attention æ£€æµ‹ - æ£€æŸ¥ GPU æ¶æ„
    flash_attn_available = False
    if torch.cuda.get_device_capability()[0] >= 8:  # Ampere æˆ–æ›´æ–°
        try:
            import flash_attn
            print(f"   Flash Attention: {flash_attn.__version__} âœ…")
            flash_attn_available = True
        except ImportError:
            print("   Flash Attention: âš ï¸ æœªå®‰è£… (å°†ä½¿ç”¨ SDPA)")
    else:
        print("   Flash Attention: âš ï¸ GPU ä¸æ”¯æŒ (éœ€è¦ Ampere+)")
    
    # BF16/TF32 æ”¯æŒ
    if torch.cuda.is_bf16_supported():
        print("   BF16: âœ… æ”¯æŒ")
    if torch.cuda.get_device_capability()[0] >= 8:
        print("   TF32: âœ… æ”¯æŒ")
    
    print("=" * 60)
    return flash_attn_available


# ============================================================
# é…ç½®å‚æ•°
# ============================================================

def parse_args():
    parser = argparse.ArgumentParser(description="GPT-2 Chinese Pretraining (A100 ä¼˜åŒ–ç‰ˆ v2)")
    
    # è·¯å¾„é…ç½®
    parser.add_argument("--work_dir", type=str, default="/root/autodl-tmp/gpt2-chinese",
                       help="å·¥ä½œç›®å½•")
    parser.add_argument("--cache_dir", type=str, default="/root/autodl-tmp/cache",
                       help="HuggingFace ç¼“å­˜ç›®å½•")
    
    # æ¨¡å‹é…ç½® (Tensor Core å¯¹é½: ç»´åº¦ä¸º 8 çš„å€æ•°)
    parser.add_argument("--vocab_size", type=int, default=32000, help="è¯è¡¨å¤§å°")
    parser.add_argument("--n_positions", type=int, default=1024, help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--n_embd", type=int, default=768, help="éšè—å±‚ç»´åº¦")
    parser.add_argument("--n_layer", type=int, default=8, help="å±‚æ•°")
    parser.add_argument("--n_head", type=int, default=12, help="æ³¨æ„åŠ›å¤´æ•°")
    
    # è®­ç»ƒé…ç½®
    parser.add_argument("--batch_size", type=int, default=48, 
                       help="æ¯ GPU æ‰¹é‡å¤§å° (A100-40GB=48, A100-80GB=64)")
    parser.add_argument("--gradient_accumulation", type=int, default=2,
                       help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--num_epochs", type=int, default=2, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning_rate", type=float, default=3e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--warmup_steps", type=int, default=2000, help="é¢„çƒ­æ­¥æ•°")
    
    # A100 ä¼˜åŒ–é…ç½®
    parser.add_argument("--use_tf32", action="store_true", default=True,
                       help="ä½¿ç”¨ TF32 åŠ é€Ÿ")
    parser.add_argument("--use_bf16", action="store_true", default=True,
                       help="ä½¿ç”¨ BF16 æ··åˆç²¾åº¦")
    parser.add_argument("--use_flash_attn", action="store_true", default=True,
                       help="ä½¿ç”¨ Flash Attention 2")
    parser.add_argument("--use_compile", action="store_true", default=True,
                       help="ä½¿ç”¨ torch.compile åŠ é€Ÿ")
    parser.add_argument("--compile_mode", type=str, default="default",
                        choices=["default", "reduce-overhead", "max-autotune"],
                        help="torch.compile æ¨¡å¼ (default å…¼å®¹ Liger Kernel)")
    parser.add_argument("--use_liger", action="store_true", default=True,
                       help="ä½¿ç”¨ Liger Kernel ä¼˜åŒ–")
    parser.add_argument("--use_8bit_adam", action="store_true", default=True,
                       help="ä½¿ç”¨ 8-bit AdamW (å¦‚å¯ç”¨)")
    
    # HuggingFace é…ç½®
    parser.add_argument("--hf_token", type=str, default=None,
                       help="HuggingFace Token")
    parser.add_argument("--push_to_hub", action="store_true", default=True,
                       help="è®­ç»ƒå®Œæˆåä¸Šä¼ åˆ° Hub")
    parser.add_argument("--hub_model_id", type=str, default=None,
                       help="Hub æ¨¡å‹ ID")
    
    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--resume", action="store_true", help="ä» checkpoint æ¢å¤")
    
    return parser.parse_args()


# ============================================================
# å·¥å…·å‡½æ•°
# ============================================================

def set_seed(seed):
    """[FIX-12] å®Œæ•´éšæœºç§å­è®¾ç½®"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def get_local_rank():
    """è·å–æœ¬åœ° rank"""
    return int(os.environ.get("LOCAL_RANK", 0))


def get_world_size():
    """è·å– world size"""
    return int(os.environ.get("WORLD_SIZE", 1))


def is_main_process():
    """æ˜¯å¦ä¸ºä¸»è¿›ç¨‹"""
    return get_local_rank() == 0


def print_rank0(msg):
    """åªåœ¨ä¸»è¿›ç¨‹æ‰“å°"""
    if is_main_process():
        print(msg)


# ============================================================
# æ•°æ®åŠ è½½ [FIX-8: å¹¶è¡Œä¸‹è½½]
# ============================================================

def load_datasets(args):
    """åŠ è½½å¹¶åˆå¹¶æ•°æ®é›†"""
    from datasets import load_dataset, concatenate_datasets
    
    print_rank0("ğŸ“¥ åŠ è½½æ•°æ®é›†...")
    
    # [FIX-8] å¹¶è¡Œä¸‹è½½
    def load_wiki():
        return load_dataset(
            "pleisto/wikipedia-cn-20230720-filtered",
            split="train",
            cache_dir=args.cache_dir,
        )
    
    def load_zhihu():
        return load_dataset(
            "wangrui6/Zhihu-KOL",
            split="train",
            cache_dir=args.cache_dir,
        )
    
    with ThreadPoolExecutor(max_workers=2) as executor:
        wiki_future = executor.submit(load_wiki)
        zhihu_future = executor.submit(load_zhihu)
        
        wiki = wiki_future.result()
        zhihu = zhihu_future.result()
    
    print_rank0(f"   âœ… ç»´åŸºç™¾ç§‘: {len(wiki)} æ¡")
    print_rank0(f"   âœ… çŸ¥ä¹: {len(zhihu)} æ¡")
    
    # ç»Ÿä¸€å­—æ®µå
    def process_wiki(example):
        return {"text": example["completion"]}
    
    def process_zhihu(example):
        return {"text": f"{example['INSTRUCTION']}\n{example['RESPONSE']}"}
    
    # [FIX-5] ä½¿ç”¨æ›´å¤š CPU æ ¸å¿ƒ
    num_proc = min(8, multiprocessing.cpu_count())
    
    wiki_processed = wiki.map(process_wiki, remove_columns=wiki.column_names, num_proc=num_proc)
    zhihu_processed = zhihu.map(process_zhihu, remove_columns=zhihu.column_names, num_proc=num_proc)
    
    # åˆå¹¶
    dataset = concatenate_datasets([wiki_processed, zhihu_processed])
    dataset = dataset.shuffle(seed=args.seed)
    
    print_rank0(f"âœ… æ•°æ®é›†åˆå¹¶å®Œæˆ: {len(dataset)} æ¡")
    return dataset


# ============================================================
# åˆ†è¯å™¨ [FIX-7: åŠ é€Ÿè®­ç»ƒ]
# ============================================================

def train_or_load_tokenizer(args, dataset):
    """è®­ç»ƒæˆ–åŠ è½½åˆ†è¯å™¨"""
    import sentencepiece as spm
    from transformers import LlamaTokenizerFast, AutoTokenizer
    
    tokenizer_dir = Path(args.work_dir) / "tokenizer"
    sp_model_path = Path(args.work_dir) / "chinese_sp.model"
    
    # ç¼“å­˜æ£€æµ‹
    if (tokenizer_dir / "tokenizer.json").exists():
        print_rank0("âœ… æ£€æµ‹åˆ°å·²æœ‰åˆ†è¯å™¨ï¼Œä»ç¼“å­˜åŠ è½½")
        tokenizer = AutoTokenizer.from_pretrained(str(tokenizer_dir))
        print_rank0(f"   è¯è¡¨å¤§å°: {len(tokenizer)}")
        return tokenizer
    
    # åªåœ¨ä¸»è¿›ç¨‹è®­ç»ƒ
    if is_main_process():
        print_rank0("ğŸ”¤ è®­ç»ƒ SentencePiece åˆ†è¯å™¨...")
        
        # é‡‡æ ·æ•°æ®
        sample_size = min(500000, len(dataset))
        random.seed(args.seed)
        indices = random.sample(range(len(dataset)), sample_size)
        
        corpus_file = Path(args.work_dir) / "corpus.txt"
        with open(corpus_file, "w", encoding="utf-8") as f:
            for idx in indices:
                text = dataset[idx]["text"].strip()
                if 50 < len(text) < 5000:
                    f.write(text + "\n")
        
        # [FIX] éªŒè¯ corpus æ–‡ä»¶å¤§å°
        corpus_size = corpus_file.stat().st_size
        if corpus_size < 1_000_000:  # è‡³å°‘ 1MB
            raise RuntimeError(f"âŒ corpus.txt å¤ªå° ({corpus_size} bytes)ï¼æ•°æ®é›†å¯èƒ½ä¸‹è½½ä¸å®Œæ•´ã€‚è¯·æ£€æŸ¥ç½‘ç»œå¹¶é‡æ–°è¿è¡Œã€‚")
        print_rank0(f"   ğŸ“„ Corpus æ–‡ä»¶: {corpus_size / 1e6:.1f} MB")
        
        # [FIX-7] ä½¿ç”¨ input_sentence_size åŠ é€Ÿ
        spm.SentencePieceTrainer.train(
            input=str(corpus_file),
            model_prefix=str(sp_model_path).replace(".model", ""),
            vocab_size=args.vocab_size,
            model_type="unigram",
            character_coverage=0.9995,
            pad_id=0, unk_id=1, bos_id=2, eos_id=3,
            pad_piece="<pad>", unk_piece="<unk>",
            bos_piece="<s>", eos_piece="</s>",
            num_threads=multiprocessing.cpu_count() or 4,
            input_sentence_size=200000,  # [FIX-7] é™åˆ¶è®­ç»ƒæ ·æœ¬
            shuffle_input_sentence=True,  # [FIX-7] æ‰“ä¹±
        )
        
        # éªŒè¯ SP æ¨¡å‹
        sp = spm.SentencePieceProcessor()
        sp.load(str(sp_model_path))
        sp_vocab_size = sp.get_piece_size()
        print_rank0(f"   ğŸ“Š SentencePiece è¯è¡¨: {sp_vocab_size} tokens")
        
        if sp_vocab_size < 1000:
            raise RuntimeError(f"âŒ SP æ¨¡å‹è¯è¡¨å¼‚å¸¸: åªæœ‰ {sp_vocab_size} tokensï¼")
        
        # [FIX-FINAL] ä½¿ç”¨ tokenizers åº“ç›´æ¥ä» SP æ¨¡å‹æ„å»º tokenizer.json
        # ç„¶åç”¨ PreTrainedTokenizerFast åŠ è½½ - è¿™æ˜¯æœ€å¯é çš„æ–¹æ³•
        tokenizer_dir.mkdir(parents=True, exist_ok=True)
        
        from tokenizers import Tokenizer, decoders, pre_tokenizers
        from tokenizers.models import Unigram
        from tokenizers.trainers import UnigramTrainer
        import json
        
        # è¯»å– SP vocab æ–‡ä»¶æ„å»º tokenizers æ ¼å¼
        vocab_path = Path(args.work_dir) / "chinese_sp.vocab"
        vocab_list = []
        with open(vocab_path, "r", encoding="utf-8") as f:
            for line in f:
                parts = line.strip().split("\t")
                if len(parts) >= 2:
                    token = parts[0]
                    score = float(parts[1])
                    vocab_list.append((token, score))
        
        print_rank0(f"   ğŸ“Š ä» vocab æ–‡ä»¶è¯»å–: {len(vocab_list)} tokens")
        
        # æ„å»º tokenizers Unigram æ¨¡å‹
        # æ³¨æ„ï¼šSentencePiece vocab ä¸­ unk_id=1 (pad=0, unk=1, bos=2, eos=3)
        tokenizer_obj = Tokenizer(Unigram(vocab_list, unk_id=1))
        tokenizer_obj.decoder = decoders.Metaspace()
        tokenizer_obj.pre_tokenizer = pre_tokenizers.Metaspace()
        
        # ä¿å­˜ä¸º tokenizer.json
        tokenizer_obj.save(str(tokenizer_dir / "tokenizer.json"))
        
        # åˆ›å»ºé…ç½®æ–‡ä»¶
        tokenizer_config = {
            "bos_token": "<s>",
            "eos_token": "</s>", 
            "unk_token": "<unk>",
            "pad_token": "<pad>",
            "add_bos_token": False,
            "add_eos_token": True,
            "model_max_length": 1024,
            "tokenizer_class": "PreTrainedTokenizerFast"
        }
        with open(tokenizer_dir / "tokenizer_config.json", "w") as f:
            json.dump(tokenizer_config, f, indent=2, ensure_ascii=False)
        
        special_tokens_map = {
            "bos_token": "<s>",
            "eos_token": "</s>",
            "unk_token": "<unk>", 
            "pad_token": "<pad>"
        }
        with open(tokenizer_dir / "special_tokens_map.json", "w") as f:
            json.dump(special_tokens_map, f, indent=2, ensure_ascii=False)
        
        # ä½¿ç”¨ PreTrainedTokenizerFast åŠ è½½
        from transformers import PreTrainedTokenizerFast
        tokenizer = PreTrainedTokenizerFast(
            tokenizer_file=str(tokenizer_dir / "tokenizer.json"),
            bos_token="<s>", eos_token="</s>",
            unk_token="<unk>", pad_token="<pad>",
        )
        tokenizer.save_pretrained(str(tokenizer_dir))
        
        actual_vocab_size = len(tokenizer)
        print_rank0(f"   ğŸ“Š HuggingFace è¯è¡¨: {actual_vocab_size} tokens")
        
        if actual_vocab_size < 1000:
            raise RuntimeError(f"âŒ åˆ†è¯å™¨è¯è¡¨å¼‚å¸¸: {actual_vocab_size} tokensï¼Œè¯·æ£€æŸ¥ç¯å¢ƒ")
        
        # ä¸Šä¼ åˆ° Hub
        if args.push_to_hub and args.hf_token:
            try:
                from huggingface_hub import HfApi
                api = HfApi(token=args.hf_token)
                user = api.whoami()["name"]
                tokenizer.push_to_hub(f"{user}/chinese-sp-32k", token=args.hf_token)
                print_rank0("ğŸš€ åˆ†è¯å™¨å·²ä¸Šä¼ è‡³ HuggingFace Hub")
            except Exception as e:
                print_rank0(f"âš ï¸ ä¸Šä¼ å¤±è´¥: {e}")
        
        print_rank0(f"âœ… åˆ†è¯å™¨è®­ç»ƒå®Œæˆï¼Œè¯è¡¨å¤§å°: {actual_vocab_size}")
    
    # ç­‰å¾…ä¸»è¿›ç¨‹
    if dist.is_initialized():
        dist.barrier()
    
    tokenizer = AutoTokenizer.from_pretrained(str(tokenizer_dir))
    return tokenizer


# ============================================================
# æ•°æ®é¢„å¤„ç† [FIX-9: é™ä½ batch_size]
# ============================================================

def prepare_dataset(args, dataset, tokenizer):
    """Tokenize å’Œ Packing"""
    from datasets import load_from_disk
    
    lm_dataset_path = Path(args.work_dir) / "lm_dataset"
    
    # ç¼“å­˜æ£€æµ‹
    if lm_dataset_path.exists():
        print_rank0("âœ… æ£€æµ‹åˆ°å·²å¤„ç†çš„æ•°æ®é›†ï¼Œä»ç¼“å­˜åŠ è½½")
        lm_dataset = load_from_disk(str(lm_dataset_path))
        print_rank0(f"   æ ·æœ¬æ•°: {len(lm_dataset)}")
        return lm_dataset
    
    # åªåœ¨ä¸»è¿›ç¨‹å¤„ç†
    if is_main_process():
        print_rank0("ğŸ”„ Tokenize æ•°æ®...")
        
        def tokenize_function(examples):
            return tokenizer(
                examples["text"],
                add_special_tokens=True,
                truncation=False,
                return_attention_mask=False,
            )
        
        num_proc = min(8, multiprocessing.cpu_count())
        
        tokenized = dataset.map(
            tokenize_function,
            batched=True,
            batch_size=2000,  # [FIX-9] é™ä½ batch_size
            remove_columns=dataset.column_names,
            num_proc=num_proc,
            desc="Tokenizing",
        )
        
        print_rank0("ğŸ“¦ Packing æ•°æ®...")
        block_size = args.n_positions
        
        def group_texts(examples):
            concatenated = {k: list(chain.from_iterable(examples[k])) for k in examples.keys()}
            total_length = len(concatenated["input_ids"])
            total_length = (total_length // block_size) * block_size
            
            result = {
                k: [t[i:i+block_size] for i in range(0, total_length, block_size)]
                for k, t in concatenated.items()
            }
            result["labels"] = result["input_ids"].copy()
            return result
        
        lm_dataset = tokenized.map(
            group_texts,
            batched=True,
            batch_size=2000,  # [FIX-9] é™ä½ batch_size
            num_proc=num_proc,
            desc="Packing",
        )
        
        lm_dataset.save_to_disk(str(lm_dataset_path))
        print_rank0(f"âœ… æ•°æ®å¤„ç†å®Œæˆ: {len(lm_dataset)} æ ·æœ¬")
    
    # ç­‰å¾…ä¸»è¿›ç¨‹
    if dist.is_initialized():
        dist.barrier()
    
    lm_dataset = load_from_disk(str(lm_dataset_path))
    return lm_dataset


# ============================================================
# æ¨¡å‹åˆ›å»º [FIX-1,2,3: ä¿®å¤ compile å’Œ DDP å†²çª]
# ============================================================

def create_model(args, tokenizer, flash_attn_available):
    """åˆ›å»ºæ¨¡å‹ï¼ˆä¸åŒ…å« compile - ç”± Trainer å¤„ç†ï¼‰"""
    from transformers import GPT2Config, GPT2LMHeadModel
    
    print_rank0("ğŸ—ï¸ åˆ›å»ºæ¨¡å‹...")
    
    config = GPT2Config(
        vocab_size=len(tokenizer),
        n_positions=args.n_positions,
        n_ctx=args.n_positions,
        n_embd=args.n_embd,
        n_layer=args.n_layer,
        n_head=args.n_head,
        n_inner=args.n_embd * 4,
        activation_function="gelu_new",
        resid_pdrop=0.0,
        embd_pdrop=0.0,
        attn_pdrop=0.0,
        bos_token_id=tokenizer.bos_token_id,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id,
    )
    
    # [FIX-13] Flash Attention æ£€æµ‹åŸºäº GPU æ¶æ„
    if args.use_flash_attn and flash_attn_available:
        config._attn_implementation = "flash_attention_2"
        print_rank0("   âœ… Flash Attention 2 å·²å¯ç”¨")
    else:
        config._attn_implementation = "sdpa"
        print_rank0("   â„¹ï¸ ä½¿ç”¨ SDPA æ³¨æ„åŠ›")
    
    model = GPT2LMHeadModel(config)
    param_count = sum(p.numel() for p in model.parameters()) / 1e6
    print_rank0(f"   âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ: {param_count:.1f}M å‚æ•°")
    
    # Liger Kernel ä¼˜åŒ–
    if args.use_liger:
        try:
            from liger_kernel.transformers import LigerLayerNorm
            import torch.nn as nn
            
            count = 0
            for name, module in list(model.named_modules()):
                if isinstance(module, nn.LayerNorm):
                    parent_name = '.'.join(name.split('.')[:-1])
                    child_name = name.split('.')[-1]
                    parent = model.get_submodule(parent_name) if parent_name else model
                    liger_ln = LigerLayerNorm(module.normalized_shape, eps=module.eps)
                    liger_ln.weight = module.weight
                    if module.bias is not None:
                        liger_ln.bias = module.bias
                    setattr(parent, child_name, liger_ln)
                    count += 1
            print_rank0(f"   âœ… Liger LayerNorm: æ›¿æ¢äº† {count} ä¸ª")
        except ImportError:
            print_rank0("   âš ï¸ liger-kernel æœªå®‰è£…ï¼Œè·³è¿‡")
    
    # [FIX-1] ä¸åœ¨è¿™é‡Œå¯ç”¨ gradient_checkpointing
    # è®© TrainingArguments ç»Ÿä¸€ç®¡ç†ï¼Œé¿å…ä¸ torch.compile å†²çª
    
    # [FIX-2,3] ä¸åœ¨è¿™é‡Œ torch.compile æˆ– DDP
    # è®© Trainer è‡ªåŠ¨å¤„ç†
    
    return model


# ============================================================
# æ£€æµ‹ bitsandbytes å¯ç”¨æ€§ [FIX-4]
# ============================================================

def get_optimizer_name(use_8bit_adam):
    """[FIX-4] å®‰å…¨æ£€æµ‹ bitsandbytes å¹¶é€‰æ‹©ä¼˜åŒ–å™¨"""
    if not use_8bit_adam:
        return "adamw_torch_fused"
    
    try:
        import bitsandbytes as bnb
        # å®é™…æµ‹è¯•æ˜¯å¦æ”¯æŒ GPU
        test_param = torch.zeros(1, device='cuda', requires_grad=True)
        bnb.optim.Adam8bit([test_param])
        print_rank0("   âœ… 8-bit AdamW å¯ç”¨")
        return "adamw_bnb_8bit"
    except Exception as e:
        print_rank0(f"   âš ï¸ 8-bit AdamW ä¸å¯ç”¨: {e}")
        print_rank0("   â†’ å›é€€åˆ° fused AdamW")
        return "adamw_torch_fused"


# ============================================================
# è®­ç»ƒ [FIX-1,2,3,5,6: å…¨é¢ä¿®å¤]
# ============================================================

def train(args):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    local_rank = get_local_rank()
    world_size = get_world_size()
    
    # [FIX-12] è®¾ç½®å®Œæ•´éšæœºç§å­
    set_seed(args.seed)
    
    # [FIX-15] æ›´æ–°ç¼“å­˜è·¯å¾„
    os.environ["HF_HOME"] = args.cache_dir
    os.environ["TRANSFORMERS_CACHE"] = args.cache_dir
    
    # ç¯å¢ƒæ£€æŸ¥ ([FIX-13] è¿”å› flash_attn å¯ç”¨æ€§)
    flash_attn_available = check_environment(local_rank)
    
    # A100 ä¼˜åŒ– ([FIX-10] ä¼ å…¥ local_rank)
    if args.use_tf32:
        enable_a100_optimizations(local_rank)
    
    print_rank0("=" * 60)
    print_rank0("ğŸš€ GPT-2 Chinese Pretraining (A100 v2 - FIXED)")
    print_rank0("=" * 60)
    print_rank0(f"   GPU: {torch.cuda.get_device_name(local_rank)}")
    print_rank0(f"   World Size: {world_size}")
    print_rank0(f"   å·¥ä½œç›®å½•: {args.work_dir}")
    
    # åˆ›å»ºç›®å½•
    Path(args.work_dir).mkdir(parents=True, exist_ok=True)
    Path(args.cache_dir).mkdir(parents=True, exist_ok=True)
    
    # HuggingFace ç™»å½•
    if args.hf_token:
        from huggingface_hub import login
        login(token=args.hf_token)
    elif os.environ.get("HF_TOKEN"):
        args.hf_token = os.environ["HF_TOKEN"]
        from huggingface_hub import login
        login(token=args.hf_token)
    
    # åŠ è½½æ•°æ®
    dataset = load_datasets(args)
    
    # åˆ†è¯å™¨
    tokenizer = train_or_load_tokenizer(args, dataset)
    
    # æ•°æ®é¢„å¤„ç†
    lm_dataset = prepare_dataset(args, dataset, tokenizer)
    
    # åˆ›å»ºæ¨¡å‹ ([FIX-1,2,3] ä¸åœ¨è¿™é‡Œ compile æˆ– DDP)
    model = create_model(args, tokenizer, flash_attn_available or False)
    
    # [FIX-4] æ£€æµ‹ä¼˜åŒ–å™¨
    optim_name = get_optimizer_name(args.use_8bit_adam)
    
    # è®­ç»ƒå‚æ•°
    from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling
    
    effective_batch = args.batch_size * args.gradient_accumulation * world_size
    print_rank0(f"\nğŸ“Š è®­ç»ƒé…ç½®:")
    print_rank0(f"   Batch/GPU: {args.batch_size}")
    print_rank0(f"   æœ‰æ•ˆ Batch: {effective_batch}")
    print_rank0(f"   Epochs: {args.num_epochs}")
    print_rank0(f"   ä¼˜åŒ–å™¨: {optim_name}")
    
    # [FIX-5,6] è®¡ç®—æœ€ä½³ num_workers
    num_workers = min(8, multiprocessing.cpu_count())
    
    training_args = TrainingArguments(
        output_dir=str(Path(args.work_dir) / "checkpoints"),
        
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation,
        
        learning_rate=args.learning_rate,
        warmup_steps=args.warmup_steps,
        lr_scheduler_type="cosine",
        
        optim=optim_name,  # [FIX-4] åŠ¨æ€é€‰æ‹©
        weight_decay=0.1,
        max_grad_norm=1.0,
        
        # BF16/FP16
        fp16=not args.use_bf16 and not torch.cuda.is_bf16_supported(),
        bf16=args.use_bf16 and torch.cuda.is_bf16_supported(),
        tf32=args.use_tf32,
        
        # [FIX-1] Gradient Checkpointing ç”± Trainer ç®¡ç†
        # åœ¨ä½¿ç”¨ torch.compile æ—¶ç¦ç”¨ï¼Œé¿å…å†²çª
        gradient_checkpointing=not args.use_compile,
        
        # [FIX-2,3] torch.compile ç”± Trainer ç®¡ç†
        torch_compile=args.use_compile,
        torch_compile_mode=args.compile_mode,
        
        eval_strategy="steps",
        eval_steps=500,
        save_strategy="steps",
        save_steps=500,
        save_total_limit=3,
        # prediction_loss_only=False,  # ç§»é™¤ä»¥æ˜¾ç¤º eval_loss
        
        logging_steps=10,
        logging_first_step=True,
        report_to="none",
        
        push_to_hub=args.push_to_hub and is_main_process(),
        hub_model_id=args.hub_model_id,
        hub_strategy="checkpoint",
        
        # [FIX-5] å¢åŠ  num_workers
        dataloader_num_workers=num_workers,
        dataloader_pin_memory=True,
        # [FIX-6] å¯ç”¨ persistent_workers
        dataloader_persistent_workers=True if num_workers > 0 else False,
        
        seed=args.seed,
        
        # DDP è®¾ç½® - [FIX-3] è®© Trainer è‡ªåŠ¨å¤„ç†
        ddp_find_unused_parameters=False,
    )
    
    # Data Collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )
    
    # Callbacks
    from transformers import TrainerCallback
    
    class GenerationCallback(TrainerCallback):
        """è¯„ä¼°æ—¶æµ‹è¯•ç”Ÿæˆè´¨é‡"""
        def __init__(self, tokenizer, prompts=None):
            self.tokenizer = tokenizer
            self.prompts = prompts or ["ä¸­å›½çš„å†å²", "äººå·¥æ™ºèƒ½æ˜¯", "ä»Šå¤©å¤©æ°”"]
        
        def on_evaluate(self, args, state, control, model, **kwargs):
            if not is_main_process():
                return
            
            print("\n" + "=" * 50)
            print(f"ğŸ“ Step {state.global_step} - ç”Ÿæˆæµ‹è¯•:")
            print("=" * 50)
            
            eval_model = model.module if hasattr(model, 'module') else model
            # å¤„ç† torch.compile åŒ…è£…
            if hasattr(eval_model, '_orig_mod'):
                eval_model = eval_model._orig_mod
            
            eval_model.eval()
            device = next(eval_model.parameters()).device
            
            for prompt in self.prompts:
                try:
                    inputs = self.tokenizer(prompt, return_tensors="pt").to(device)
                    with torch.no_grad():
                        outputs = eval_model.generate(
                            **inputs, max_new_tokens=50,
                            do_sample=True, temperature=0.8, top_k=50,
                            pad_token_id=self.tokenizer.pad_token_id,
                        )
                    generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                    print(f"   [{prompt}] â†’ {generated[:80]}...")
                except Exception as e:
                    print(f"   [{prompt}] â†’ ç”Ÿæˆå¤±è´¥: {e}")
            print("=" * 50 + "\n")
    
    class DetailedLoggingCallback(TrainerCallback):
        """è¯¦ç»†æ—¥å¿—"""
        def on_log(self, args, state, control, logs=None, **kwargs):
            if not is_main_process() or not logs:
                return
            
            step = state.global_step
            train_loss = logs.get("loss")
            eval_loss = logs.get("eval_loss")
            lr = logs.get("learning_rate", 0)
            
            # æ˜¾ç¤ºè®­ç»ƒ loss
            if train_loss is not None and (step <= 10 or step % 100 == 0):
                print(f"ğŸ“Š Step {step}: train_loss={train_loss:.4f}, lr={lr:.2e}")
            
            # æ˜¾ç¤ºéªŒè¯ loss (eval æ—¶è§¦å‘)
            if eval_loss is not None:
                print(f"ï¿½ Step {step}: eval_loss={eval_loss:.4f}")
    
    callbacks = [DetailedLoggingCallback()]
    if is_main_process():
        callbacks.append(GenerationCallback(tokenizer))
    
    # [FIX-14] éšæœºé‡‡æ · eval_dataset
    eval_indices = random.sample(range(len(lm_dataset)), min(1000, len(lm_dataset)))
    eval_dataset = lm_dataset.select(eval_indices)
    
    # åˆ›å»º Trainer
    # [FIX-2,3] ç›´æ¥ä¼ å…¥ modelï¼Œè®© Trainer å¤„ç† DDP å’Œ compile
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=lm_dataset,
        eval_dataset=eval_dataset,
        processing_class=tokenizer,  # transformers 5.0: tokenizer -> processing_class
        data_collator=data_collator,
        callbacks=callbacks,
    )
    
    # [FIX] æ£€æŸ¥ resume æ—¶æ˜¯å¦æœ‰ checkpoint
    resume_path = None
    if args.resume:
        checkpoint_dir = Path(args.work_dir) / "checkpoints"
        checkpoints = list(checkpoint_dir.glob("checkpoint-*")) if checkpoint_dir.exists() else []
        if checkpoints:
            resume_path = True  # Trainer ä¼šè‡ªåŠ¨æ‰¾æœ€æ–°çš„
            print_rank0(f"âœ… æ£€æµ‹åˆ° {len(checkpoints)} ä¸ª checkpointï¼Œç»§ç»­è®­ç»ƒ")
        else:
            print_rank0("âš ï¸ æœªæ‰¾åˆ° checkpointï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
    
    # å¼€å§‹è®­ç»ƒ
    print_rank0("\n" + "=" * 60)
    print_rank0("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
    print_rank0("=" * 60)
    
    try:
        trainer.train(resume_from_checkpoint=resume_path)
        print_rank0("\nâœ… è®­ç»ƒå®Œæˆ!")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        if is_main_process():
            final_path = Path(args.work_dir) / "final_model"
            trainer.save_model(str(final_path))
            tokenizer.save_pretrained(str(final_path))
            
            # ä¸Šä¼ åˆ° Hub
            if args.push_to_hub and args.hf_token:
                print_rank0("ğŸ“¤ ä¸Šä¼ åˆ° HuggingFace Hub...")
                from huggingface_hub import HfApi
                api = HfApi(token=args.hf_token)
                user = api.whoami()["name"]
                repo_id = args.hub_model_id or f"{user}/gpt2-chinese-mini"
                
                # [FIX] å…ˆåˆ›å»ºä»“åº“ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                api.create_repo(repo_id=repo_id, exist_ok=True, private=False)
                
                api.upload_folder(
                    folder_path=str(final_path),
                    repo_id=repo_id,
                    commit_message="Training complete (A100 v2)",
                )
                print_rank0(f"ğŸ‰ æ¨¡å‹å·²ä¸Šä¼ è‡³: https://huggingface.co/{repo_id}")
                
    except KeyboardInterrupt:
        print_rank0("\nâš ï¸ è®­ç»ƒè¢«ä¸­æ–­ï¼Œä¿å­˜ checkpoint...")
        trainer.save_model()


# ============================================================
# ä¸»å…¥å£
# ============================================================

if __name__ == "__main__":
    args = parse_args()
    train(args)
