# === Cell 1: ç¯å¢ƒè¯Šæ–­ä¸åº“å®‰è£… ===
import os
# å¯ç”¨åŒ GPU
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"
# æ£€æŸ¥ GPU å¯ç”¨æ€§
!nvidia-smi
# å®‰è£…ä¾èµ–ï¼ˆä¸æŒ‡å®šç‰ˆæœ¬ï¼Œä½¿ç”¨ Kaggle é¢„è£…çš„å…¼å®¹ç‰ˆæœ¬ï¼‰
!pip install -q \
    transformers datasets accelerate \
    huggingface_hub sentencepiece tokenizers \
    bitsandbytes \
    liger-kernel
# éªŒè¯å®‰è£…
import torch
print(f"PyTorch: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU count: {torch.cuda.device_count()}")
for i in range(torch.cuda.device_count()):
    print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
# æ³¨æ„ï¼šKaggle ç¯å¢ƒä¸æ”¯æŒ DDPï¼ŒTrainer ä¼šè‡ªåŠ¨ä½¿ç”¨ DataParallel


# === Cell 2: HuggingFace Hub ç™»å½• ===
from huggingface_hub import notebook_login, HfFolder
# æ–¹æ³• 1: äº¤äº’å¼ç™»å½• (éœ€è¦æ‰‹åŠ¨è¾“å…¥ token)
notebook_login()
# æ–¹æ³• 2: éäº¤äº’å¼ (ç”¨äº "Save & Run All" æ¨¡å¼)
# import os
# os.environ["HF_TOKEN"] = "hf_your_token_here"
# HfFolder.save_token(os.environ["HF_TOKEN"])
# éªŒè¯ç™»å½•çŠ¶æ€
from huggingface_hub import whoami
try:
    user_info = whoami()
    HF_USERNAME = user_info["name"]
    print(f"âœ… å·²ç™»å½•ä¸º: {HF_USERNAME}")
except Exception as e:
    print(f"âŒ ç™»å½•å¤±è´¥: {e}")
    HF_USERNAME = "your_username"  # éœ€æ‰‹åŠ¨å¡«å†™


    # === Cell 3: å¤šæ•°æ®é›†åŠ è½½ï¼ˆæ¨èï¼‰===
from datasets import load_dataset, concatenate_datasets, Dataset
import os
print("ğŸ“¥ æ­£åœ¨ä¸‹è½½å¤šä¸ªæ•°æ®é›†...")
# === 1. åŠ è½½ç»´åŸºç™¾ç§‘ï¼ˆå®Œæ•´ï¼‰===
print("   [1/2] åŠ è½½ç»´åŸºç™¾ç§‘...")
wiki = load_dataset(
    "pleisto/wikipedia-cn-20230720-filtered",
    split="train",  # å®Œæ•´æ•°æ®
)
print(f"   âœ… ç»´åŸºç™¾ç§‘: {len(wiki)} æ¡")
# === 2. åŠ è½½çŸ¥ä¹é«˜èµå›ç­” ===
print("   [2/2] åŠ è½½çŸ¥ä¹...")
zhihu = load_dataset(
    "wangrui6/Zhihu-KOL",
    split="train",
)
print(f"   âœ… çŸ¥ä¹: {len(zhihu)} æ¡")
# === 3. ç»Ÿä¸€å­—æ®µåå¹¶åˆå¹¶ ===
# ç»´åŸºç™¾ç§‘å­—æ®µ: completion
# çŸ¥ä¹å­—æ®µ: INSTRUCTION, RESPONSE (éœ€è¦åˆå¹¶)
def process_wiki(example):
    return {"text": example["completion"]}
def process_zhihu(example):
    # åˆå¹¶é—®é¢˜å’Œå›ç­”
    text = f"{example['INSTRUCTION']}\n{example['RESPONSE']}"
    return {"text": text}
print("ğŸ”„ æ­£åœ¨å¤„ç†æ•°æ®æ ¼å¼...")
wiki_processed = wiki.map(process_wiki, remove_columns=wiki.column_names, num_proc=4)
zhihu_processed = zhihu.map(process_zhihu, remove_columns=zhihu.column_names, num_proc=4)
# åˆå¹¶æ•°æ®é›†
dataset = concatenate_datasets([wiki_processed, zhihu_processed])
dataset = dataset.shuffle(seed=42)
print(f"\nâœ… æ•°æ®é›†åˆå¹¶å®Œæˆ!")
print(f"   ç»´åŸºç™¾ç§‘: {len(wiki)} æ¡")
print(f"   çŸ¥ä¹: {len(zhihu)} æ¡")
print(f"   åˆè®¡: {len(dataset)} æ¡")
print(f"   ç¤ºä¾‹: {dataset[0]['text'][:100]}...")
# === 4. å¯¼å‡ºä¸ºçº¯æ–‡æœ¬ä¾›åˆ†è¯å™¨è®­ç»ƒï¼ˆä¼˜åŒ–ç‰ˆï¼‰===
CORPUS_FILE = "/kaggle/working/combined_corpus.txt"
print("ğŸ“ æ­£åœ¨å¯¼å‡ºçº¯æ–‡æœ¬ï¼ˆä¼˜åŒ–ç‰ˆï¼‰...")
# ä½¿ç”¨ map + æ‰¹é‡å†™å…¥ï¼Œæ¯” for å¾ªç¯å¿« 5-10 å€
def filter_short(example):
    return len(example["text"].strip()) > 50
# è¿‡æ»¤çŸ­æ–‡æœ¬
filtered = dataset.filter(filter_short, num_proc=4)
print(f"   è¿‡æ»¤å: {len(filtered)} æ¡ (åŸ {len(dataset)} æ¡)")
# æ‰¹é‡å†™å…¥æ–‡ä»¶
BATCH_SIZE = 10000
with open(CORPUS_FILE, "w", encoding="utf-8") as f:
    for i in range(0, len(filtered), BATCH_SIZE):
        batch = filtered[i:i+BATCH_SIZE]["text"]
        f.write("\n".join(text.strip() for text in batch) + "\n")
        if (i // BATCH_SIZE) % 10 == 0:
            print(f"   å·²å¯¼å‡º {min(i+BATCH_SIZE, len(filtered))}/{len(filtered)}...")
# ç»Ÿè®¡è¯­æ–™è§„æ¨¡
file_size_mb = os.path.getsize(CORPUS_FILE) / (1024 * 1024)
with open(CORPUS_FILE, "r", encoding="utf-8") as f:
    line_count = sum(1 for _ in f)
print(f"âœ… è¯­æ–™å¯¼å‡ºå®Œæˆ: {file_size_mb:.1f} MB, {line_count} è¡Œ")


# === Cell 4: è®­ç»ƒ SentencePiece åˆ†è¯å™¨ï¼ˆå¸¦ç¼“å­˜æ£€æµ‹ï¼‰===
import sentencepiece as spm
import os
import random
import multiprocessing
import time

MODEL_PREFIX = "/kaggle/working/chinese_sp"
VOCAB_SIZE = 32000
SAMPLE_SIZE = 500000

# === ç¼“å­˜æ£€æµ‹ï¼šå¦‚æœåˆ†è¯å™¨å·²å­˜åœ¨åˆ™è·³è¿‡ ===
if os.path.exists(f"{MODEL_PREFIX}.model") and os.path.exists(f"{MODEL_PREFIX}.vocab"):
    print("=" * 50)
    print("âœ… æ£€æµ‹åˆ°å·²è®­ç»ƒçš„åˆ†è¯å™¨ï¼Œè·³è¿‡è®­ç»ƒ")
    print(f"   æ¨¡å‹æ–‡ä»¶: {MODEL_PREFIX}.model")
    print(f"   è¯è¡¨æ–‡ä»¶: {MODEL_PREFIX}.vocab")
    # éªŒè¯åˆ†è¯å™¨
    sp = spm.SentencePieceProcessor(model_file=f"{MODEL_PREFIX}.model")
    test_text = "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ"
    tokens = sp.encode(test_text, out_type=str)
    print(f"   æµ‹è¯•åˆ†è¯: {test_text} â†’ {tokens}")
    print("=" * 50)
else:
    # === è®­ç»ƒæ–°åˆ†è¯å™¨ ===
    print(f"ğŸ“ é‡‡æ · {SAMPLE_SIZE} å¥ç”¨äºåˆ†è¯å™¨è®­ç»ƒ...")
    print(f"   CPU æ ¸å¿ƒæ•°: {multiprocessing.cpu_count()}")
    
    random.seed(42)
    indices = list(range(len(dataset)))
    random.shuffle(indices)
    sampled_indices = indices[:SAMPLE_SIZE]
    
    sampled_dataset = dataset.select(sampled_indices)
    
    def filter_text(example):
        text = example["text"].strip()
        if 50 < len(text) < 5000:
            return {"text": text, "valid": True}
        return {"text": "", "valid": False}
    
    print("ğŸ”„ å¹¶è¡Œå¤„ç†æ–‡æœ¬...")
    processed = sampled_dataset.map(
        filter_text,
        num_proc=multiprocessing.cpu_count(),
        desc="Processing"
    )
    
    valid_texts = [ex["text"] for ex in processed if ex["valid"]]
    print(f"âœ… æœ‰æ•ˆæ–‡æœ¬: {len(valid_texts)} å¥")
    
    with open(CORPUS_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(valid_texts))
    corpus_size_mb = os.path.getsize(CORPUS_FILE) / 1024 / 1024
    print(f"âœ… è¯­æ–™å¯¼å‡ºå®Œæˆ: {corpus_size_mb:.1f} MB")
    
    # è®­ç»ƒåˆ†è¯å™¨
    print("=" * 50)
    print("ğŸ”¤ å¼€å§‹è®­ç»ƒåˆ†è¯å™¨...")
    print(f"   è¯è¡¨å¤§å°: {VOCAB_SIZE}")
    print(f"   è®­ç»ƒå¥æ•°: {len(valid_texts)}")
    print(f"   CPU çº¿ç¨‹: {os.cpu_count() or 4}")
    print("   â³ é¢„è®¡è€—æ—¶ 2-5 åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
    print("=" * 50)
    
    start_time = time.time()
    spm.SentencePieceTrainer.train(
        input=CORPUS_FILE,
        model_prefix=MODEL_PREFIX,
        vocab_size=VOCAB_SIZE,
        model_type="unigram",
        character_coverage=0.9995,
        input_sentence_size=500000,
        shuffle_input_sentence=True,
        max_sentence_length=1024,
        train_extremely_large_corpus=False,
        pad_id=0, unk_id=1, bos_id=2, eos_id=3,
        pad_piece="<pad>", unk_piece="<unk>",
        bos_piece="<s>", eos_piece="</s>",
        num_threads=os.cpu_count() or 4,
        normalization_rule_name="nmt_nfkc_cf",
        split_by_unicode_script=True,
        split_by_number=True,
    )
    elapsed = time.time() - start_time
    print("=" * 50)
    print(f"âœ… åˆ†è¯å™¨è®­ç»ƒå®Œæˆ! è€—æ—¶: {elapsed:.1f} ç§’")
    print(f"   æ¨¡å‹æ–‡ä»¶: {MODEL_PREFIX}.model")
    print(f"   è¯è¡¨æ–‡ä»¶: {MODEL_PREFIX}.vocab")
    
    # éªŒè¯åˆ†è¯å™¨
    sp = spm.SentencePieceProcessor(model_file=f"{MODEL_PREFIX}.model")
    test_text = "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ"
    tokens = sp.encode(test_text, out_type=str)
    print(f"   æµ‹è¯•åˆ†è¯: {test_text} â†’ {tokens}")


# === Cell 5: è½¬æ¢åˆ†è¯å™¨ä¸º HuggingFace æ ¼å¼ï¼ˆå¸¦ç¼“å­˜æ£€æµ‹ï¼‰===
import sentencepiece as spm
from transformers import PreTrainedTokenizerFast, LlamaTokenizerFast, AutoTokenizer
import json
import os
import shutil

TOKENIZER_DIR = "/kaggle/working/tokenizer"

# === ç¼“å­˜æ£€æµ‹ï¼šå¦‚æœ HF æ ¼å¼åˆ†è¯å™¨å·²å­˜åœ¨åˆ™è·³è¿‡ ===
if os.path.exists(f"{TOKENIZER_DIR}/tokenizer.json"):
    print("=" * 50)
    print("âœ… æ£€æµ‹åˆ°å·²è½¬æ¢çš„ HuggingFace åˆ†è¯å™¨ï¼Œè·³è¿‡è½¬æ¢")
    print(f"   ç›®å½•: {TOKENIZER_DIR}")
    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)
    print(f"   è¯è¡¨å¤§å°: {len(tokenizer)}")
    # æµ‹è¯•åˆ†è¯
    test_text = "äººå·¥æ™ºèƒ½æ˜¯æœªæ¥ç§‘æŠ€å‘å±•çš„é‡è¦æ–¹å‘ã€‚"
    tokens = tokenizer.tokenize(test_text)
    print(f"   æµ‹è¯•: {test_text[:20]}... â†’ {tokens[:5]}...")
    print("=" * 50)
else:
    # === è½¬æ¢åˆ†è¯å™¨ ===
    os.makedirs(TOKENIZER_DIR, exist_ok=True)
    
    # å¤åˆ¶æ¨¡å‹æ–‡ä»¶
    shutil.copy(f"{MODEL_PREFIX}.model", f"{TOKENIZER_DIR}/spiece.model")
    shutil.copy(f"{MODEL_PREFIX}.vocab", f"{TOKENIZER_DIR}/spiece.vocab")
    
    # åˆ›å»º tokenizer_config.json
    tokenizer_config = {
        "bos_token": "<s>",
        "eos_token": "</s>",
        "unk_token": "<unk>",
        "pad_token": "<pad>",
        "sp_model_kwargs": {},
        "add_bos_token": False,
        "add_eos_token": False,
        "model_max_length": 512,
        "tokenizer_class": "PreTrainedTokenizerFast"
    }
    with open(f"{TOKENIZER_DIR}/tokenizer_config.json", "w") as f:
        json.dump(tokenizer_config, f, indent=2, ensure_ascii=False)
    
    # åˆ›å»º special_tokens_map.json
    special_tokens = {
        "bos_token": "<s>",
        "eos_token": "</s>",
        "unk_token": "<unk>",
        "pad_token": "<pad>"
    }
    with open(f"{TOKENIZER_DIR}/special_tokens_map.json", "w") as f:
        json.dump(special_tokens, f, indent=2, ensure_ascii=False)
    
    # ä½¿ç”¨ LlamaTokenizerFast åŠ è½½ SP æ¨¡å‹
    tokenizer = LlamaTokenizerFast(
        vocab_file=f"{MODEL_PREFIX}.model",
        bos_token="<s>",
        eos_token="</s>",
        unk_token="<unk>",
        pad_token="<pad>",
        add_bos_token=False,
        add_eos_token=True,
    )
    
    # ä¿å­˜ä¸ºæ ‡å‡† HuggingFace æ ¼å¼
    tokenizer.save_pretrained(TOKENIZER_DIR)
    print(f"âœ… åˆ†è¯å™¨è½¬æ¢å®Œæˆ!")
    print(f"   ç›®å½•: {TOKENIZER_DIR}")
    print(f"   è¯è¡¨å¤§å°: {len(tokenizer)}")
    
    # === ä¸Šä¼ åˆ†è¯å™¨åˆ° HuggingFace Hub ===
    TOKENIZER_REPO = f"{HF_USERNAME}/chinese-sp-32k"
    try:
        tokenizer.push_to_hub(TOKENIZER_REPO)
        print(f"\nğŸš€ åˆ†è¯å™¨å·²ä¸Šä¼ è‡³: https://huggingface.co/{TOKENIZER_REPO}")
        print(f"   ä½¿ç”¨æ–¹å¼: AutoTokenizer.from_pretrained('{TOKENIZER_REPO}')")
    except Exception as e:
        print(f"âš ï¸ ä¸Šä¼ å¤±è´¥: {e}")
        print("   å¯ç¨åæ‰‹åŠ¨ä¸Šä¼ : tokenizer.push_to_hub('ä½ çš„ä»“åº“å')")
    
    # æµ‹è¯•åˆ†è¯æ•ˆæœ
    test_text = "äººå·¥æ™ºèƒ½æ˜¯æœªæ¥ç§‘æŠ€å‘å±•çš„é‡è¦æ–¹å‘ã€‚"
    tokens = tokenizer.tokenize(test_text)
    ids = tokenizer.encode(test_text)
    print(f"\næµ‹è¯•åˆ†è¯:")
    print(f"   åŸæ–‡: {test_text}")
    print(f"   Tokens: {tokens}")
    print(f"   IDs: {ids}")
    print(f"   è§£ç : {tokenizer.decode(ids)}")


# === Cell 6: GPT-2 æ¨¡å‹é…ç½®ä¸åˆå§‹åŒ– ===
from transformers import GPT2Config, GPT2LMHeadModel
# é‡æ–°åŠ è½½åˆ†è¯å™¨ (ç¡®ä¿ä½¿ç”¨æœ€æ–°ä¿å­˜çš„ç‰ˆæœ¬)
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)
# === æ¨¡å‹é…ç½®ï¼ˆä¼˜åŒ–ç‰ˆï¼‰===
config = GPT2Config(
    vocab_size=len(tokenizer),
    
    # === æ¶æ„ï¼ˆä¼˜åŒ–åï¼‰===
    n_positions=1024,                   # å¢å¤§ contextï¼ˆæ˜¾å­˜ä¼˜åŒ–åæ”¯æŒï¼‰
    n_ctx=1024,
    n_embd=768,
    n_layer=6,
    n_head=12,
    n_inner=3072,
    activation_function="gelu_new",
    
    # === æ­£åˆ™åŒ–ï¼ˆé¢„è®­ç»ƒé˜¶æ®µå…³é—­ dropoutï¼‰===
    resid_pdrop=0.0,
    embd_pdrop=0.0,
    attn_pdrop=0.0,
    
    # === ç‰¹æ®Š Token ID ===
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id,
)
# === ä¼˜åŒ– 0: SDPA Attention ===
try:
    config._attn_implementation = "sdpa"
    print("âœ… å·²è®¾ç½® SDPA attentionï¼ˆMemory-Efficient åç«¯ï¼‰")
except:
    pass
# åˆå§‹åŒ–æ¨¡å‹ (éšæœºæƒé‡)
model = GPT2LMHeadModel(config)
print(f"âœ… å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M")
# === ä¼˜åŒ– 1: Liger Kernel LayerNormï¼ˆ+30% é€Ÿåº¦ï¼‰===
try:
    from liger_kernel.transformers import LigerLayerNorm
    import torch.nn as nn
    
    def patch_layernorm(model):
        patched_count = 0
        for name, module in list(model.named_modules()):
            if isinstance(module, nn.LayerNorm):
                parent_name = '.'.join(name.split('.')[:-1])
                child_name = name.split('.')[-1]
                parent = model.get_submodule(parent_name) if parent_name else model
                liger_ln = LigerLayerNorm(module.normalized_shape, eps=module.eps)
                liger_ln.weight = module.weight
                if module.bias is not None:
                    liger_ln.bias = module.bias
                setattr(parent, child_name, liger_ln)
                patched_count += 1
        return model, patched_count
    
    model, count = patch_layernorm(model)
    print(f"âœ… å·²æ›¿æ¢ {count} ä¸ª LayerNorm ä¸º Liger ç‰ˆæœ¬")
except ImportError:
    print("âš ï¸ liger-kernel æœªå®‰è£…ï¼Œè·³è¿‡ LayerNorm ä¼˜åŒ–")
# ä¿å­˜ lm_head.weightï¼ˆä¾› FusedLinearCrossEntropy ä½¿ç”¨ï¼‰
LM_HEAD_WEIGHT = model.lm_head.weight

# === ä¼˜åŒ– 2: torch.compile ===
# æ³¨æ„ï¼štorch.compile å’Œ DataParallel ä¸å…¼å®¹ï¼Œé€‰æ‹© DataParallelï¼ˆä½¿ç”¨åŒ GPUï¼‰
# å¦‚æœåªæœ‰å• GPUï¼Œå¯ä»¥å¯ç”¨ torch.compile
USE_COMPILE = False  # DataParallel æ¨¡å¼ä¸‹å¿…é¡»ç¦ç”¨
if USE_COMPILE and torch.cuda.device_count() == 1:
    try:
        model = torch.compile(model)
        print("âœ… å·²å¯ç”¨ torch.compile")
    except Exception as e:
        print(f"âš ï¸ torch.compile ä¸å¯ç”¨: {e}")

# === ä¼˜åŒ– 3: Gradient Checkpointingï¼ˆåœ¨ DataParallel ä¹‹å‰å¯ç”¨ï¼‰===
# å¿…é¡»åœ¨åŒ…è£… DataParallel ä¹‹å‰å¯ç”¨ï¼Œå¦åˆ™ Trainer ä¼šæŠ¥é”™
model.gradient_checkpointing_enable()
print("âœ… å·²å¯ç”¨ Gradient Checkpointing")

# å¼ºåˆ¶ä½¿ç”¨ DataParallelï¼ˆåŒ GPUï¼‰
if torch.cuda.device_count() > 1:
    print(f"ğŸ® å¯ç”¨ DataParallelï¼Œä½¿ç”¨ {torch.cuda.device_count()} ä¸ª GPU")
    model = torch.nn.DataParallel(model)

# === Cell 7: æ•°æ®é¢„å¤„ç†ä¸ Packingï¼ˆå¸¦ç¼“å­˜æ£€æµ‹ï¼‰===
from datasets import Dataset, load_from_disk
import numpy as np
import multiprocessing
import time
from itertools import chain

NUM_PROC = multiprocessing.cpu_count()
BLOCK_SIZE = 1024
LM_DATASET_PATH = "/kaggle/working/lm_dataset"

# === ç¼“å­˜æ£€æµ‹ï¼šå¦‚æœå¤„ç†åçš„æ•°æ®é›†å·²å­˜åœ¨åˆ™è·³è¿‡ ===
if os.path.exists(LM_DATASET_PATH):
    print("=" * 50)
    print("âœ… æ£€æµ‹åˆ°å·²å¤„ç†çš„æ•°æ®é›†ï¼Œä»ç¼“å­˜åŠ è½½")
    print(f"   è·¯å¾„: {LM_DATASET_PATH}")
    lm_dataset = load_from_disk(LM_DATASET_PATH)
    print(f"   æ ·æœ¬æ•°: {len(lm_dataset)} ä¸ª {BLOCK_SIZE}-token å—")
    print(f"   æ€» Token: {len(lm_dataset) * BLOCK_SIZE / 1e9:.2f}B")
    # éªŒè¯æ•°æ®
    sample = lm_dataset[0]
    print(f"   è§£ç : {tokenizer.decode(sample['input_ids'][:50])}...")
    print("=" * 50)
else:
    print(f"ğŸ–¥ï¸ ä½¿ç”¨ {NUM_PROC} ä¸ª CPU æ ¸å¿ƒå¹¶è¡Œå¤„ç†")
    
    # === æ­¥éª¤ 1: Tokenize æ‰€æœ‰æ–‡æœ¬ ===
    print("=" * 50)
    print("ğŸ”„ æ­£åœ¨ Tokenize...")
    tokenize_start = time.time()
    
    text_column = "text" if "text" in dataset.column_names else "completion"
    print(f"   å­—æ®µ: {text_column}")
    print(f"   æ ·æœ¬æ•°: {len(dataset)}")
    
    def tokenize_function(examples):
        return tokenizer(
            examples[text_column],
            add_special_tokens=True,
            truncation=False,
            return_attention_mask=False,
        )
    
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        batch_size=5000,
        remove_columns=dataset.column_names,
        num_proc=NUM_PROC,
        desc="Tokenizing",
    )
    tokenize_time = time.time() - tokenize_start
    print(f"âœ… Tokenization å®Œæˆ: {len(tokenized_dataset)} æ ·æœ¬, è€—æ—¶ {tokenize_time:.1f}s")
    
    # === æ­¥éª¤ 2: Packing ===
    print("=" * 50)
    print("ğŸ“¦ æ­£åœ¨ Packing...")
    pack_start = time.time()
    
    def group_texts(examples):
        concatenated = {k: list(chain.from_iterable(examples[k])) for k in examples.keys()}
        total_length = len(concatenated["input_ids"])
        
        if total_length >= BLOCK_SIZE:
            total_length = (total_length // BLOCK_SIZE) * BLOCK_SIZE
        
        result = {
            k: [t[i : i + BLOCK_SIZE] for i in range(0, total_length, BLOCK_SIZE)]
            for k, t in concatenated.items()
        }
        result["labels"] = result["input_ids"].copy()
        return result
    
    lm_dataset = tokenized_dataset.map(
        group_texts,
        batched=True,
        batch_size=5000,
        num_proc=NUM_PROC,
        desc="Packing",
    )
    pack_time = time.time() - pack_start
    
    # ä¿å­˜åˆ°ç£ç›˜ä½œä¸ºç¼“å­˜
    lm_dataset.save_to_disk(LM_DATASET_PATH)
    
    total_time = tokenize_time + pack_time
    print("=" * 50)
    print(f"âœ… æ•°æ®å¤„ç†å®Œæˆ!")
    print(f"   Tokenize: {tokenize_time:.1f}s")
    print(f"   Packing: {pack_time:.1f}s")
    print(f"   æ€»è€—æ—¶: {total_time:.1f}s")
    print(f"   æ ·æœ¬æ•°: {len(lm_dataset)} ä¸ª {BLOCK_SIZE}-token å—")
    print(f"   æ€» Token: {len(lm_dataset) * BLOCK_SIZE / 1e9:.2f}B")
    print(f"   å·²ç¼“å­˜è‡³: {LM_DATASET_PATH}")
    
    # éªŒè¯æ•°æ®
    sample = lm_dataset[0]
    print(f"\nğŸ“Š æ•°æ®æ ·æœ¬:")
    print(f"   è§£ç : {tokenizer.decode(sample['input_ids'][:50])}...")


# === Cell 8: è®­ç»ƒå‚æ•°é…ç½® ===
from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling
# === æ¨¡å‹å‘½å ===
MODEL_NAME = "gpt2-chinese-mini"
REPO_ID = f"{HF_USERNAME}/{MODEL_NAME}"
# === è®­ç»ƒå‚æ•°ï¼ˆæ˜¾å­˜ä¼˜åŒ–ç‰ˆï¼‰===
NUM_EPOCHS = 2
BATCH_SIZE_PER_GPU = 16              # DataParallel æ¨¡å¼ä¸‹é™ä½ batch size
GRADIENT_ACCUMULATION = 2
NUM_GPUS = 2
BLOCK_SIZE = 1024
# æœ‰æ•ˆ batch size = 24 * 2 * 2 = 96 samples
# æ¯ä¸ª sample 1024 tokens = ~98K tokens per update
effective_batch_size = BATCH_SIZE_PER_GPU * GRADIENT_ACCUMULATION * NUM_GPUS
tokens_per_update = effective_batch_size * BLOCK_SIZE
steps_per_epoch = len(lm_dataset) // effective_batch_size
total_steps = steps_per_epoch * NUM_EPOCHS
print(f"ğŸ“Š è®­ç»ƒè§„æ¨¡:")
print(f"   æ¯ GPU batch: {BATCH_SIZE_PER_GPU}")
print(f"   æ¢¯åº¦ç´¯ç§¯: {GRADIENT_ACCUMULATION}")
print(f"   æœ‰æ•ˆ batch: {effective_batch_size}")
print(f"   æ¯æ­¥ tokens: {tokens_per_update / 1e3:.1f}K")
print(f"   æ€»æ­¥æ•°: {total_steps}")
# === è®­ç»ƒå‚æ•° ===
training_args = TrainingArguments(
    output_dir=f"/kaggle/working/{MODEL_NAME}",
    overwrite_output_dir=True,
    
    # === è®­ç»ƒè§„æ¨¡ ===
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE_PER_GPU,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION,
    
    # === å­¦ä¹ ç‡è°ƒåº¦ï¼ˆé’ˆå¯¹å¤§ batch ä¼˜åŒ–ï¼‰===
    learning_rate=3e-4,                  # å¤§ batch æ—¶é™ä½ LR
    warmup_steps=2000,                   # çº¦ 5% æ€»æ­¥æ•°
    lr_scheduler_type="cosine",
    
    # === ä¼˜åŒ–å™¨ï¼ˆ8-bit ä¼˜åŒ–ï¼Œæ˜¾å­˜å‡å°‘ 75%ï¼‰===
    optim="adamw_bnb_8bit",
    weight_decay=0.1,
    adam_beta1=0.9,
    adam_beta2=0.95,
    max_grad_norm=1.0,
    
    # === ç²¾åº¦ ===
    fp16=True,
    bf16=False,                          # T4 ä¸æ”¯æŒ
    gradient_checkpointing=False,        # å·²åœ¨ Cell 6 æ‰‹åŠ¨å¯ç”¨ï¼ˆDataParallel å‰ï¼‰
    
    # === æ•°æ®åŠ è½½ä¼˜åŒ– ===
    dataloader_num_workers=4,
    dataloader_pin_memory=True,
    dataloader_prefetch_factor=4,
    dataloader_persistent_workers=True,
    
    # === Checkpoint ===
    eval_strategy="steps",
    eval_steps=500,
    save_strategy="steps",
    save_steps=500,
    save_total_limit=3,
    load_best_model_at_end=False,
    prediction_loss_only=True,    # å¼ºåˆ¶è®¡ç®— eval loss
    
    # === æ—¥å¿—ä¸ Hub ===
    logging_steps=1,               # å‰å‡ æ­¥æ¯æ­¥æ‰“å°ï¼Œç¡®è®¤æ— è¯¯åæ”¹ä¸º 100
    logging_first_step=True,
    report_to="none",
    push_to_hub=True,
    hub_model_id=REPO_ID,
    hub_strategy="checkpoint",
    
    # === åˆ†å¸ƒå¼ ===
    ddp_find_unused_parameters=False,
    remove_unused_columns=False,  # torch.compile å…¼å®¹
    seed=42,
)
print(f"\nâœ… è®­ç»ƒé…ç½®å®Œæˆ")
print(f"   Warmup æ­¥æ•°: {training_args.warmup_steps}")
print(f"   LR è°ƒåº¦: {training_args.lr_scheduler_type}")
print(f"   ä¼˜åŒ–å™¨: {training_args.optim}")
print(f"   ä¿å­˜é—´éš”: {training_args.save_steps} æ­¥") 


# === Cell 9: Data Collator é…ç½® ===
# è¯­è¨€æ¨¡å‹ä¸“ç”¨ Data Collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,  # å› æœè¯­è¨€æ¨¡å‹ (é Masked LM)
)
# æµ‹è¯• collator
test_batch = [lm_dataset[i] for i in range(4)]
collated = data_collator(test_batch)
print("ğŸ“¦ Data Collator æµ‹è¯•:")
print(f"   input_ids shape: {collated['input_ids'].shape}")
print(f"   labels shape: {collated['labels'].shape}") 


# === Cell 10: è‡ªå®šä¹‰ Trainer + ç”Ÿæˆå›è°ƒ ===
from transformers import Trainer, TrainerCallback
# ç¡®ä¿ LM_HEAD_WEIGHT å­˜åœ¨ï¼ˆå¦‚æœä¹‹å‰çš„ Cell æ²¡æœ‰å®šä¹‰ï¼‰
try:
    LM_HEAD_WEIGHT
except NameError:
    # å¤„ç† DataParallel åŒ…è£…
    base_model = model.module if hasattr(model, 'module') else model
    # å¤„ç† torch.compile åŒ…è£…
    base_model = base_model._orig_mod if hasattr(base_model, '_orig_mod') else base_model
    LM_HEAD_WEIGHT = base_model.lm_head.weight
    print("âœ… å·²è·å– LM_HEAD_WEIGHT")
# === ç”Ÿæˆå›è°ƒï¼šæ¯æ¬¡è¯„ä¼°æ—¶æµ‹è¯• prompt ===
class GenerationCallback(TrainerCallback):
    def __init__(self, tokenizer, prompts=None):
        self.tokenizer = tokenizer
        self.prompts = prompts or ["ä¸­å›½çš„å†å²", "äººå·¥æ™ºèƒ½æ˜¯", "çŸ¥ä¹ä¸Šæœ‰äººé—®"]
    
    def on_evaluate(self, args, state, control, model, **kwargs):
        print("\n" + "=" * 50)
        print(f"ğŸ“ Step {state.global_step} - ç”Ÿæˆæ ·æœ¬:")
        print("=" * 50)
        
        # å¤„ç† DataParallel å’Œ torch.compile åŒ…è£…
        eval_model = model
        if hasattr(model, 'module'):  # DataParallel
            eval_model = model.module
        if hasattr(eval_model, '_orig_mod'):  # torch.compile
            eval_model = eval_model._orig_mod
        
        eval_model.eval()
        device = next(eval_model.parameters()).device
        for prompt in self.prompts:
            try:
                inputs = self.tokenizer(prompt, return_tensors="pt").to(device)
                with torch.no_grad():
                    outputs = eval_model.generate(
                        **inputs, max_new_tokens=50,
                        do_sample=True, temperature=0.8, top_k=50,
                        pad_token_id=self.tokenizer.pad_token_id,
                    )
                generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                print(f"  [{prompt}] â†’ {generated}")
            except Exception as e:
                print(f"  [{prompt}] â†’ ç”Ÿæˆå¤±è´¥: {e}")
        print("=" * 50 + "\n")
        eval_model.train()
# === æ—¥å¿—å›è°ƒï¼šå‰ 10 æ­¥æ¯æ­¥æ‰“å°ï¼Œä¹‹åæ¯ 100 æ­¥ ===
class LoggingCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs:
            loss = logs.get("loss", logs.get("eval_loss", None))
            if loss is not None:
                step = state.global_step
                # å‰ 10 æ­¥æ¯æ­¥æ‰“å°ï¼Œä¹‹åæ¯ 100 æ­¥æ‰“å°
                if step <= 10 or step % 100 == 0:
                    lr = logs.get("learning_rate", 0)
                    print(f"ğŸ“Š Step {step}: loss={loss:.4f}, lr={lr:.2e}")
# === LigerTrainer: ä½¿ç”¨ FusedLinearCrossEntropy ===
USE_FUSED_CE = False  # è®¾ç½®ä¸º False å¯ç¦ç”¨
try:
    from liger_kernel.transformers import LigerFusedLinearCrossEntropyLoss
    
    class LigerTrainer(Trainer):
        """ä½¿ç”¨ FusedLinearCrossEntropyï¼Œæ˜¾å­˜ -60~80%"""
        def __init__(self, *args, lm_head_weight=None, **kwargs):
            super().__init__(*args, **kwargs)
            self.fused_loss_fn = LigerFusedLinearCrossEntropyLoss()
            self.lm_head_weight = lm_head_weight
            print("âœ… LigerTrainer: å·²å¯ç”¨ FusedLinearCrossEntropy")
        
        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
            labels = inputs.pop("labels")
            base_model = getattr(model, '_orig_mod', model)
            transformer = getattr(base_model, 'transformer', None)
            outputs = transformer(
                input_ids=inputs["input_ids"],
                attention_mask=inputs.get("attention_mask"),
            )
            hidden_states = outputs.last_hidden_state
            shift_hidden = hidden_states[..., :-1, :].contiguous().view(-1, hidden_states.size(-1))
            shift_labels = labels[..., 1:].contiguous().view(-1)
            loss = self.fused_loss_fn(self.lm_head_weight, shift_hidden, shift_labels)
            if return_outputs:
                from transformers.modeling_outputs import CausalLMOutputWithPast
                return loss, CausalLMOutputWithPast(loss=loss)
            return loss
    
    if USE_FUSED_CE:
        TrainerClass = LigerTrainer
        trainer_kwargs = {"lm_head_weight": LM_HEAD_WEIGHT}
        print("âœ… å°†ä½¿ç”¨ LigerTrainer + FusedLinearCrossEntropy")
    else:
        TrainerClass = Trainer
        trainer_kwargs = {}
except ImportError:
    print("âš ï¸ liger-kernel æœªå®‰è£…ï¼Œä½¿ç”¨æ ‡å‡† Trainer")
    TrainerClass = Trainer
    trainer_kwargs = {}
# === åˆ›å»º Trainer ===
generation_callback = GenerationCallback(
    tokenizer=tokenizer,
    prompts=["ä¸­å›½çš„å†å²", "äººå·¥æ™ºèƒ½æ˜¯", "çŸ¥ä¹ä¸Šæœ‰äººé—®", "ç§‘å­¦æŠ€æœ¯çš„å‘å±•"]
)
logging_callback = LoggingCallback()
trainer = TrainerClass(
    model=model,
    args=training_args,
    train_dataset=lm_dataset,
    eval_dataset=lm_dataset.select(range(min(1000, len(lm_dataset)))),
    tokenizer=tokenizer,
    data_collator=data_collator,
    callbacks=[generation_callback, logging_callback],  # ä¸¤ä¸ªå›è°ƒ
    **trainer_kwargs,
)
print("ğŸ¯ Trainer åˆ›å»ºå®Œæˆ!")


# === Cell 11: å¤š GPU è®­ç»ƒï¼ˆDataParallel æ¨¡å¼ï¼‰===
# æ³¨æ„ï¼šKaggle ç¯å¢ƒä¸æ”¯æŒ DDPï¼ŒTrainer ä¼šè‡ªåŠ¨ä½¿ç”¨ DataParallel
import time
import gc
import torch
# æ¸…ç† GPU ç¼“å­˜
gc.collect()
torch.cuda.empty_cache()
print("=" * 60)
print("ğŸš€ å¼€å§‹å¤š GPU é¢„è®­ç»ƒ")
print("=" * 60)
print(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
print(f"ğŸ–¥ï¸ GPU æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f} GB å·²ç”¨")
print(f"ğŸ® GPU æ•°é‡: {torch.cuda.device_count()}")
print(f"ğŸ“ æ¨¡å‹å°†ä¸Šä¼ è‡³: https://huggingface.co/{REPO_ID}")
print()
print("ğŸ“ˆ Loss å‚è€ƒ:")
print("   åˆå§‹ (éšæœºæƒé‡): ~10.0 - 11.0")
print("   500 æ­¥å: ~6.0 - 7.0")
print("   æ”¶æ•›ç›®æ ‡: < 4.0")
print("=" * 60)
# === å…³é”®æ£€æŸ¥ï¼švocab_size ä¸€è‡´æ€§ ===
# å¤„ç† DataParallel åŒ…è£…
base_model = model.module if hasattr(model, 'module') else model
model_vocab = base_model.config.vocab_size
tokenizer_vocab = len(tokenizer)
print(f"\nğŸ” vocab_size æ£€æŸ¥:")
print(f"   æ¨¡å‹: {model_vocab}")
print(f"   åˆ†è¯å™¨: {tokenizer_vocab}")
if model_vocab != tokenizer_vocab:
    print(f"âŒ é”™è¯¯: vocab_size ä¸åŒ¹é…! æ¨¡å‹={model_vocab}, åˆ†è¯å™¨={tokenizer_vocab}")
    print("   è¯·é‡æ–°è¿è¡Œ Cell 6 åˆ›å»ºæ¨¡å‹ï¼Œç¡®ä¿ä½¿ç”¨ vocab_size=len(tokenizer)")
    raise ValueError(f"vocab_size mismatch: model={model_vocab}, tokenizer={tokenizer_vocab}")
# æ£€æŸ¥æ•´ä¸ªæ•°æ®é›†çš„ token ID èŒƒå›´ï¼ˆæŠ½æ ·æ£€æŸ¥ï¼‰
print("   æ£€æŸ¥æ•°æ®é›† token ID èŒƒå›´...")
max_token_id = 0
problem_samples = []
for i in range(0, len(lm_dataset), max(1, len(lm_dataset) // 100)):  # æŠ½æ · 100 ä¸ª
    sample_ids = lm_dataset[i]["input_ids"]
    sample_max = max(sample_ids)
    if sample_max > max_token_id:
        max_token_id = sample_max
    if sample_max >= model_vocab:
        problem_samples.append((i, sample_max))
print(f"   æ•°æ®é›†æœ€å¤§ token ID: {max_token_id}")
print(f"   æ¨¡å‹ vocab_size: {model_vocab}")
if problem_samples:
    print(f"âŒ å‘ç° {len(problem_samples)} ä¸ªæ ·æœ¬çš„ token ID è¶Šç•Œ!")
    for idx, max_id in problem_samples[:5]:
        print(f"   æ ·æœ¬ {idx}: max_token_id = {max_id} >= {model_vocab}")
    print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
    print("   1. åˆ é™¤ /kaggle/working/tokenizer ç›®å½•")
    print("   2. é‡æ–°è¿è¡Œ Cell 4 (è®­ç»ƒåˆ†è¯å™¨)")
    print("   3. é‡æ–°è¿è¡Œ Cell 5 (ä¿å­˜åˆ†è¯å™¨)")
    print("   4. é‡æ–°è¿è¡Œ Cell 6-10")
    raise ValueError(f"Token IDs out of range!")
if max_token_id >= model_vocab:
    print(f"âŒ é”™è¯¯: token ID {max_token_id} >= vocab_size {model_vocab}")
    raise ValueError(f"Token ID out of range: {max_token_id} >= {model_vocab}")
print("âœ… vocab_size æ£€æŸ¥é€šè¿‡!\n")
# === å¼€å§‹è®­ç»ƒ ===
try:
    trainer.train()
    print("\nâœ… è®­ç»ƒå®Œæˆ!")
    
    # ä¿å­˜æ¨¡å‹
    trainer.save_model()
    tokenizer.save_pretrained(f"/kaggle/working/{MODEL_NAME}")
    
    # ä¸Šä¼ åˆ° Hub
    print("\nğŸ“¤ æ­£åœ¨ä¸Šä¼ æœ€ç»ˆæ¨¡å‹åˆ° HuggingFace Hub...")
    from huggingface_hub import HfApi
    api = HfApi()
    api.upload_folder(
        folder_path=f"/kaggle/working/{MODEL_NAME}",
        repo_id=REPO_ID,
        commit_message="Training complete"
    )
    print(f"\nğŸ‰ æ¨¡å‹å·²å‘å¸ƒè‡³: https://huggingface.co/{REPO_ID}")
    
except KeyboardInterrupt:
    print("\nâš ï¸ è®­ç»ƒè¢«ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜å½“å‰çŠ¶æ€...")
    trainer.save_model()
    print("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ï¼Œä¸‹æ¬¡å¯ç”¨ resume_from_checkpoint=True ç»§ç»­")
    
except Exception as e:
    print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
    print("\nğŸ’¡ æ’é”™æŒ‡å—:")
    print("   - OOM: å‡å°‘ BATCH_SIZE_PER_GPU")
    print("   - é€Ÿåº¦æ…¢: æ£€æŸ¥ gradient_checkpointing")
    print("   - Loss ä¸é™: æ£€æŸ¥æ•°æ®å’Œ learning_rate")
    raise