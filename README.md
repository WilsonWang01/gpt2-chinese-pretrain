# ğŸ‡¨ğŸ‡³ ä¸­æ–‡ GPT-2 ä»é›¶é¢„è®­ç»ƒ

ä»éšæœºæƒé‡å¼€å§‹ï¼Œä½¿ç”¨ä¸­æ–‡ç»´åŸºç™¾ç§‘å’ŒçŸ¥ä¹æ•°æ®è®­ç»ƒä¸€ä¸ªä¸­æ–‡ GPT-2 è¯­è¨€æ¨¡å‹ã€‚

[![Demo](https://img.shields.io/badge/ğŸ¤—%20Demo-Gradio-yellow)](https://huggingface.co/spaces/Wilsonwin/gpt2-chinese-demo)
[![Model](https://img.shields.io/badge/ğŸ¤—%20Model-HuggingFace-blue)](https://huggingface.co/Wilsonwin/gpt2-chinese-pretrained)

## ğŸ“Š è®­ç»ƒç»“æœ

| æŒ‡æ ‡ | å€¼ |
|------|-----|
| èµ·å§‹ Loss | 7.40 |
| æœ€ç»ˆ Loss | 4.25 |
| Loss ä¸‹é™ | 42.6% |
| æ€»æ­¥æ•° | 11,838 |
| è®­ç»ƒæ—¶é•¿ | ~1.5 å°æ—¶ (A100) |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼ä¸€ï¼šç›´æ¥ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("Wilsonwin/gpt2-chinese-pretrained")
model = AutoModelForCausalLM.from_pretrained("Wilsonwin/gpt2-chinese-pretrained")

text = "äººå·¥æ™ºèƒ½çš„æœªæ¥"
inputs = tokenizer(text, return_tensors="pt")
outputs = model.generate(**inputs, max_length=100, do_sample=True, temperature=0.8)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

### æ–¹å¼äºŒï¼šä»é›¶è®­ç»ƒè‡ªå·±çš„æ¨¡å‹

å‚è€ƒä¸‹æ–¹è¯¦ç»†æ•™ç¨‹ã€‚

---

## ğŸ“š è¯¦ç»†æ•™ç¨‹

### 1. ç¯å¢ƒå‡†å¤‡

#### AutoDLï¼ˆæ¨èï¼‰

1. åœ¨ [AutoDL](https://www.autodl.com/) ç§Ÿç”¨ GPU å®ä¾‹
   - æ¨èé…ç½®ï¼šA100-40GB / RTX 4090 / RTX 5090
   - é•œåƒé€‰æ‹©ï¼š`PyTorch 2.x + CUDA 12.x`

2. SSH è¿æ¥åˆ°å®ä¾‹ï¼š
```bash
ssh -p [ç«¯å£] root@[åœ°å€]
```

3. å®‰è£…ä¾èµ–ï¼š
```bash
pip install transformers datasets sentencepiece accelerate bitsandbytes
pip install flash-attn --no-build-isolation  # å¯é€‰ï¼ŒåŠ é€Ÿè®­ç»ƒ
```

#### Kaggleï¼ˆå…è´¹ï¼‰

1. åœ¨ [Kaggle](https://www.kaggle.com/) åˆ›å»º Notebook
2. å¼€å¯ GPU åŠ é€Ÿï¼ˆSettings â†’ Accelerator â†’ GPU T4 x2ï¼‰
3. å¼€å¯ç½‘ç»œè®¿é—®ï¼ˆSettings â†’ Internet â†’ Onï¼‰

### 2. è·å– HuggingFace Token

1. æ³¨å†Œ [HuggingFace](https://huggingface.co/) è´¦å·
2. è¿›å…¥ [Token è®¾ç½®é¡µé¢](https://huggingface.co/settings/tokens)
3. åˆ›å»º Tokenï¼ˆéœ€è¦ Write æƒé™ï¼‰

### 3. è¿è¡Œè®­ç»ƒ

#### AutoDL è®­ç»ƒ

```bash
# 1. ä¸Šä¼ è®­ç»ƒè„šæœ¬
scp -P [ç«¯å£] a100_train.py root@[åœ°å€]:/root/autodl-tmp/

# 2. SSH ç™»å½•åè¿è¡Œ
cd /root/autodl-tmp
source /etc/network_turbo  # å¼€å¯å­¦æœ¯åŠ é€Ÿ
export HF_TOKEN="ä½ çš„token"
python a100_train.py \
    --work_dir /root/autodl-tmp/gpt2-chinese \
    --cache_dir /root/autodl-tmp/cache \
    --batch_size 48 \
    --num_epochs 2
```

#### Kaggle è®­ç»ƒ

å¤åˆ¶ `kaggle_pretrain_combined.py` å†…å®¹åˆ° Kaggle Notebook è¿è¡Œã€‚

### 4. æ¨¡å‹éƒ¨ç½²

è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨ä¸Šä¼ åˆ° HuggingFace Hubã€‚ä½ ä¹Ÿå¯ä»¥åˆ›å»º Gradio Spaceï¼š

```python
from huggingface_hub import create_repo, upload_file

create_repo("ä½ çš„ç”¨æˆ·å/gpt2-chinese-demo", repo_type="space", space_sdk="gradio")
upload_file("app.py", "ä½ çš„ç”¨æˆ·å/gpt2-chinese-demo", repo_type="space")
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
â”œâ”€â”€ a100_train.py              # AutoDL/é«˜ç«¯GPUè®­ç»ƒè„šæœ¬ï¼ˆæ¨èï¼‰
â”œâ”€â”€ autodl_train.py            # AutoDLç®€åŒ–ç‰ˆè®­ç»ƒè„šæœ¬
â”œâ”€â”€ kaggle_pretrain_combined.py # Kaggleè®­ç»ƒè„šæœ¬
â”œâ”€â”€ hf_space_app.py            # Gradioæ¼”ç¤ºåº”ç”¨
â”œâ”€â”€ requirements.txt           # ä¾èµ–åˆ—è¡¨
â””â”€â”€ README.md                  # æœ¬æ–‡æ¡£
```

## âš™ï¸ è®­ç»ƒå‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--batch_size` | 32 | æ‰¹æ¬¡å¤§å° |
| `--num_epochs` | 2 | è®­ç»ƒè½®æ•° |
| `--learning_rate` | 3e-4 | å­¦ä¹ ç‡ |
| `--vocab_size` | 32000 | è¯è¡¨å¤§å° |
| `--n_embd` | 768 | åµŒå…¥ç»´åº¦ |
| `--n_layer` | 12 | Transformer å±‚æ•° |
| `--n_head` | 12 | æ³¨æ„åŠ›å¤´æ•° |

## ğŸ”§ å¸¸è§é—®é¢˜

### Q: è®­ç»ƒä¸­æ–­äº†æ€ä¹ˆåŠï¼Ÿ
A: ä½¿ç”¨ `--resume` å‚æ•°ä» checkpoint æ¢å¤è®­ç»ƒã€‚

### Q: æ˜¾å­˜ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ
A: 1) å‡å° `--batch_size`ï¼›2) å¼€å¯ gradient checkpointingï¼ˆé»˜è®¤å¼€å¯ï¼‰

### Q: å¦‚ä½•ä½¿ç”¨è‡ªå·±çš„æ•°æ®ï¼Ÿ
A: ä¿®æ”¹ `load_dataset_cached()` å‡½æ•°ï¼Œè¿”å›åŒ…å« "text" å­—æ®µçš„ Datasetã€‚

### Q: ä¸ºä»€ä¹ˆ Loss ä¸‹é™åˆ° 4.x å°±ä¸åŠ¨äº†ï¼Ÿ
A: è¿™æ˜¯æ­£å¸¸çš„ã€‚å°æ¨¡å‹åœ¨æœ‰é™æ•°æ®ä¸Šçš„æ”¶æ•›ç‚¹å¤§çº¦åœ¨ 4-5 ä¹‹é—´ã€‚å¢åŠ æ¨¡å‹è§„æ¨¡å’Œæ•°æ®é‡å¯ä»¥ç»§ç»­é™ä½ Lossã€‚

## ğŸ“– æŠ€æœ¯ç»†èŠ‚

- **åˆ†è¯å™¨**: SentencePiece Unigramï¼ˆ32K è¯è¡¨ï¼Œä¸“ä¸ºä¸­æ–‡ä¼˜åŒ–ï¼‰
- **æ¨¡å‹æ¶æ„**: GPT-2ï¼ˆ12å±‚ Transformer Decoderï¼‰
- **è®­ç»ƒæ•°æ®**: ä¸­æ–‡ç»´åŸºç™¾ç§‘ + çŸ¥ä¹é—®ç­”ï¼ˆçº¦ 126 ä¸‡æ¡ï¼‰
- **ä¼˜åŒ–å™¨**: AdamW 8-bitï¼ˆbitsandbytesï¼‰
- **å­¦ä¹ ç‡è°ƒåº¦**: Cosine with Warmup

## ğŸ“„ License

MIT License

## ğŸ™ è‡´è°¢

- [HuggingFace Transformers](https://github.com/huggingface/transformers)
- [AutoDL](https://www.autodl.com/)
- [Kaggle](https://www.kaggle.com/)
