#!/usr/bin/env python3
"""
GPT-2 Chinese Pretraining Script for AutoDL (5090/4090)
========================================================
ä¼˜åŒ–ç‰ˆæœ¬ï¼Œé€‚é… AutoDL ç¯å¢ƒå’Œé«˜ç«¯æ¶ˆè´¹çº§æ˜¾å¡

ä¸ Kaggle ç‰ˆæœ¬çš„ä¸»è¦åŒºåˆ«ï¼š
1. ä½¿ç”¨ DDP (DistributedDataParallel) è€Œé DataParallel
2. å¯ç”¨ torch.compile åŠ é€Ÿ (+50-100%)
3. è·¯å¾„æ”¹ä¸º AutoDL æ ‡å‡†è·¯å¾„ (/root/autodl-tmp/)
4. æ‰¹é‡å¤§å°é’ˆå¯¹ 32GB æ˜¾å­˜ä¼˜åŒ–
5. æ”¯æŒå‘½ä»¤è¡Œå‚æ•°é…ç½®

è¿è¡Œæ–¹å¼ï¼ˆå•å¡ï¼‰ï¼š
    python autodl_train.py

è¿è¡Œæ–¹å¼ï¼ˆå¤šå¡ DDPï¼‰ï¼š
    torchrun --nproc_per_node=2 autodl_train.py

ç¯å¢ƒå®‰è£…ï¼š
    pip install torch transformers datasets accelerate sentencepiece tokenizers bitsandbytes liger-kernel huggingface_hub flash-attn
"""

import os
import sys
import time
import gc
import json
import random
import argparse
import multiprocessing
from pathlib import Path
from itertools import chain

import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP


# ============================================================
# ç‰ˆæœ¬å…¼å®¹æ€§æ£€æŸ¥
# ============================================================

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒç‰ˆæœ¬å…¼å®¹æ€§"""
    print("=" * 60)
    print("ğŸ” ç¯å¢ƒæ£€æŸ¥")
    print("=" * 60)
    
    # Python ç‰ˆæœ¬
    py_version = sys.version_info
    print(f"   Python: {py_version.major}.{py_version.minor}.{py_version.micro}", end="")
    if py_version.major >= 3 and py_version.minor >= 10:
        print(" âœ…")
    else:
        print(" âš ï¸ æ¨è 3.10+")
    
    # PyTorch ç‰ˆæœ¬
    print(f"   PyTorch: {torch.__version__}", end="")
    torch_major = int(torch.__version__.split('.')[0])
    if torch_major >= 2:
        print(" âœ…")
    else:
        print(" âš ï¸ æ¨è 2.0+")
    
    # CUDA
    if torch.cuda.is_available():
        cuda_version = torch.version.cuda
        print(f"   CUDA: {cuda_version}", end="")
        if cuda_version and float(cuda_version.split('.')[0]) >= 12:
            print(" âœ… (5090 éœ€è¦ CUDA 12.8+)")
        else:
            print(f" âš ï¸ 5090 æ¨è CUDA 12.8+")
        
        # GPU ä¿¡æ¯
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_mem = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"   GPU {i}: {gpu_name} ({gpu_mem:.0f}GB)")
            
            # æ£€æµ‹ 5090/4090
            if "5090" in gpu_name:
                print(f"         â†’ Blackwell æ¶æ„ï¼ŒBF16/FlashAttn2 æ¨è âœ…")
            elif "4090" in gpu_name:
                print(f"         â†’ Ada æ¶æ„ï¼ŒBF16 æ¨è âœ…")
    else:
        print("   CUDA: âŒ æœªæ£€æµ‹åˆ° GPU!")
        sys.exit(1)
    
    # Transformers ç‰ˆæœ¬
    try:
        import transformers
        print(f"   Transformers: {transformers.__version__}", end="")
        tf_major = int(transformers.__version__.split('.')[0])
        if tf_major >= 4:
            print(" âœ…")
        else:
            print(" âš ï¸ æ¨è 4.36+")
    except ImportError:
        print("   Transformers: âŒ æœªå®‰è£…!")
        sys.exit(1)
    
    # Flash Attention
    try:
        import flash_attn
        print(f"   Flash Attention: {flash_attn.__version__} âœ…")
    except ImportError:
        print("   Flash Attention: âš ï¸ æœªå®‰è£… (å°†ä½¿ç”¨ SDPA)")
    
    # BF16 æ”¯æŒ
    if torch.cuda.is_bf16_supported():
        print("   BF16: âœ… æ”¯æŒ")
    else:
        print("   BF16: âŒ ä¸æ”¯æŒ (å°†ä½¿ç”¨ FP16)")
    
    print("=" * 60)


# ============================================================
# é…ç½®å‚æ•°
# ============================================================

def parse_args():
    parser = argparse.ArgumentParser(description="GPT-2 Chinese Pretraining")
    
    # è·¯å¾„é…ç½®
    parser.add_argument("--work_dir", type=str, default="/root/autodl-tmp/gpt2-chinese",
                       help="å·¥ä½œç›®å½•")
    parser.add_argument("--cache_dir", type=str, default="/root/autodl-tmp/cache",
                       help="HuggingFace ç¼“å­˜ç›®å½•")
    
    # æ¨¡å‹é…ç½®
    parser.add_argument("--vocab_size", type=int, default=32000, help="è¯è¡¨å¤§å°")
    parser.add_argument("--n_positions", type=int, default=1024, help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--n_embd", type=int, default=768, help="éšè—å±‚ç»´åº¦")
    parser.add_argument("--n_layer", type=int, default=6, help="å±‚æ•°")
    parser.add_argument("--n_head", type=int, default=12, help="æ³¨æ„åŠ›å¤´æ•°")
    
    # è®­ç»ƒé…ç½®
    parser.add_argument("--batch_size", type=int, default=48, 
                       help="æ¯ GPU æ‰¹é‡å¤§å° (5090 32GB å¯ç”¨ 48, 4090 24GB ç”¨ 32)")
    parser.add_argument("--gradient_accumulation", type=int, default=2,
                       help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--num_epochs", type=int, default=2, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning_rate", type=float, default=3e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--warmup_steps", type=int, default=2000, help="é¢„çƒ­æ­¥æ•°")
    
    # === 5090/Blackwell ä¼˜åŒ–é…ç½® ===
    parser.add_argument("--use_compile", action="store_true", default=True,
                       help="ä½¿ç”¨ torch.compile åŠ é€Ÿ")
    parser.add_argument("--compile_mode", type=str, default="max-autotune",
                       choices=["default", "reduce-overhead", "max-autotune"],
                       help="torch.compile æ¨¡å¼ (5090 æ¨è max-autotune)")
    parser.add_argument("--use_bf16", action="store_true", default=True,
                       help="ä½¿ç”¨ BF16 (5090/4090 æ¨èï¼Œæ¯” FP16 æ›´ç¨³å®š)")
    parser.add_argument("--use_flash_attn", action="store_true", default=True,
                       help="ä½¿ç”¨ Flash Attention 2")
    parser.add_argument("--use_liger", action="store_true", default=True,
                       help="ä½¿ç”¨ Liger Kernel ä¼˜åŒ–")
    parser.add_argument("--use_8bit_adam", action="store_true", default=True,
                       help="ä½¿ç”¨ 8-bit AdamW")
    
    # HuggingFace é…ç½®
    parser.add_argument("--hf_token", type=str, default=None,
                       help="HuggingFace Token (æˆ–è®¾ç½® HF_TOKEN ç¯å¢ƒå˜é‡)")
    parser.add_argument("--push_to_hub", action="store_true", default=True,
                       help="è®­ç»ƒå®Œæˆåä¸Šä¼ åˆ° Hub")
    parser.add_argument("--hub_model_id", type=str, default=None,
                       help="Hub æ¨¡å‹ ID (é»˜è®¤: ç”¨æˆ·å/gpt2-chinese-mini)")
    
    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--resume", action="store_true", help="ä» checkpoint æ¢å¤")
    
    return parser.parse_args()


def setup_distributed():
    """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if "LOCAL_RANK" in os.environ:
        local_rank = int(os.environ["LOCAL_RANK"])
        world_size = int(os.environ["WORLD_SIZE"])
        dist.init_process_group(backend="nccl")
        torch.cuda.set_device(local_rank)
        return local_rank, world_size, True
    else:
        return 0, 1, False


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


def is_main_process(local_rank):
    """æ˜¯å¦ä¸ºä¸»è¿›ç¨‹"""
    return local_rank == 0


def print_rank0(msg, local_rank=0):
    """åªåœ¨ä¸»è¿›ç¨‹æ‰“å°"""
    if is_main_process(local_rank):
        print(msg)


# ============================================================
# æ•°æ®åŠ è½½
# ============================================================

def load_datasets(args, local_rank):
    """åŠ è½½å¹¶åˆå¹¶æ•°æ®é›†"""
    from datasets import load_dataset, concatenate_datasets
    
    print_rank0("ğŸ“¥ åŠ è½½æ•°æ®é›†...", local_rank)
    
    # åŠ è½½ç»´åŸºç™¾ç§‘
    print_rank0("   [1/2] åŠ è½½ç»´åŸºç™¾ç§‘...", local_rank)
    wiki = load_dataset(
        "pleisto/wikipedia-cn-20230720-filtered",
        split="train",
        cache_dir=args.cache_dir,
    )
    print_rank0(f"   âœ… ç»´åŸºç™¾ç§‘: {len(wiki)} æ¡", local_rank)
    
    # åŠ è½½çŸ¥ä¹
    print_rank0("   [2/2] åŠ è½½çŸ¥ä¹...", local_rank)
    zhihu = load_dataset(
        "wangrui6/Zhihu-KOL",
        split="train",
        cache_dir=args.cache_dir,
    )
    print_rank0(f"   âœ… çŸ¥ä¹: {len(zhihu)} æ¡", local_rank)
    
    # ç»Ÿä¸€å­—æ®µå
    def process_wiki(example):
        return {"text": example["completion"]}
    
    def process_zhihu(example):
        return {"text": f"{example['INSTRUCTION']}\n{example['RESPONSE']}"}
    
    wiki_processed = wiki.map(process_wiki, remove_columns=wiki.column_names, num_proc=4)
    zhihu_processed = zhihu.map(process_zhihu, remove_columns=zhihu.column_names, num_proc=4)
    
    # åˆå¹¶
    dataset = concatenate_datasets([wiki_processed, zhihu_processed])
    dataset = dataset.shuffle(seed=args.seed)
    
    print_rank0(f"âœ… æ•°æ®é›†åˆå¹¶å®Œæˆ: {len(dataset)} æ¡", local_rank)
    return dataset


# ============================================================
# åˆ†è¯å™¨
# ============================================================

def train_or_load_tokenizer(args, dataset, local_rank):
    """è®­ç»ƒæˆ–åŠ è½½åˆ†è¯å™¨"""
    import sentencepiece as spm
    from transformers import LlamaTokenizerFast, AutoTokenizer
    
    tokenizer_dir = Path(args.work_dir) / "tokenizer"
    sp_model_path = Path(args.work_dir) / "chinese_sp.model"
    
    # ç¼“å­˜æ£€æµ‹
    if (tokenizer_dir / "tokenizer.json").exists():
        print_rank0("âœ… æ£€æµ‹åˆ°å·²æœ‰åˆ†è¯å™¨ï¼Œä»ç¼“å­˜åŠ è½½", local_rank)
        tokenizer = AutoTokenizer.from_pretrained(str(tokenizer_dir))
        print_rank0(f"   è¯è¡¨å¤§å°: {len(tokenizer)}", local_rank)
        return tokenizer
    
    # åªåœ¨ä¸»è¿›ç¨‹è®­ç»ƒåˆ†è¯å™¨
    if is_main_process(local_rank):
        print_rank0("ğŸ”¤ è®­ç»ƒ SentencePiece åˆ†è¯å™¨...", local_rank)
        
        # é‡‡æ ·æ•°æ®
        sample_size = min(500000, len(dataset))
        random.seed(args.seed)
        indices = random.sample(range(len(dataset)), sample_size)
        
        corpus_file = Path(args.work_dir) / "corpus.txt"
        with open(corpus_file, "w", encoding="utf-8") as f:
            for idx in indices:
                text = dataset[idx]["text"].strip()
                if 50 < len(text) < 5000:
                    f.write(text + "\n")
        
        # è®­ç»ƒ
        spm.SentencePieceTrainer.train(
            input=str(corpus_file),
            model_prefix=str(sp_model_path).replace(".model", ""),
            vocab_size=args.vocab_size,
            model_type="unigram",
            character_coverage=0.9995,
            pad_id=0, unk_id=1, bos_id=2, eos_id=3,
            pad_piece="<pad>", unk_piece="<unk>",
            bos_piece="<s>", eos_piece="</s>",
            num_threads=os.cpu_count() or 4,
        )
        
        # è½¬æ¢ä¸º HuggingFace æ ¼å¼
        tokenizer_dir.mkdir(parents=True, exist_ok=True)
        tokenizer = LlamaTokenizerFast(
            vocab_file=str(sp_model_path),
            bos_token="<s>", eos_token="</s>",
            unk_token="<unk>", pad_token="<pad>",
            add_bos_token=False, add_eos_token=True,
        )
        tokenizer.save_pretrained(str(tokenizer_dir))
        
        # ä¸Šä¼ åˆ° Hub
        if args.push_to_hub and args.hf_token:
            try:
                from huggingface_hub import HfApi
                api = HfApi(token=args.hf_token)
                user = api.whoami()["name"]
                tokenizer.push_to_hub(f"{user}/chinese-sp-32k", token=args.hf_token)
                print_rank0(f"ğŸš€ åˆ†è¯å™¨å·²ä¸Šä¼ è‡³ HuggingFace Hub", local_rank)
            except Exception as e:
                print_rank0(f"âš ï¸ ä¸Šä¼ å¤±è´¥: {e}", local_rank)
        
        print_rank0(f"âœ… åˆ†è¯å™¨è®­ç»ƒå®Œæˆï¼Œè¯è¡¨å¤§å°: {len(tokenizer)}", local_rank)
    
    # ç­‰å¾…ä¸»è¿›ç¨‹å®Œæˆ
    if dist.is_initialized():
        dist.barrier()
    
    # æ‰€æœ‰è¿›ç¨‹åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(str(tokenizer_dir))
    return tokenizer


# ============================================================
# æ•°æ®é¢„å¤„ç†
# ============================================================

def prepare_dataset(args, dataset, tokenizer, local_rank):
    """Tokenize å’Œ Packing"""
    from datasets import load_from_disk
    
    lm_dataset_path = Path(args.work_dir) / "lm_dataset"
    
    # ç¼“å­˜æ£€æµ‹
    if lm_dataset_path.exists():
        print_rank0("âœ… æ£€æµ‹åˆ°å·²å¤„ç†çš„æ•°æ®é›†ï¼Œä»ç¼“å­˜åŠ è½½", local_rank)
        lm_dataset = load_from_disk(str(lm_dataset_path))
        print_rank0(f"   æ ·æœ¬æ•°: {len(lm_dataset)}", local_rank)
        return lm_dataset
    
    # åªåœ¨ä¸»è¿›ç¨‹å¤„ç†
    if is_main_process(local_rank):
        print_rank0("ğŸ”„ Tokenize æ•°æ®...", local_rank)
        
        def tokenize_function(examples):
            return tokenizer(
                examples["text"],
                add_special_tokens=True,
                truncation=False,
                return_attention_mask=False,
            )
        
        tokenized = dataset.map(
            tokenize_function,
            batched=True,
            batch_size=5000,
            remove_columns=dataset.column_names,
            num_proc=multiprocessing.cpu_count(),
            desc="Tokenizing",
        )
        
        print_rank0("ğŸ“¦ Packing æ•°æ®...", local_rank)
        block_size = args.n_positions
        
        def group_texts(examples):
            concatenated = {k: list(chain.from_iterable(examples[k])) for k in examples.keys()}
            total_length = len(concatenated["input_ids"])
            total_length = (total_length // block_size) * block_size
            
            result = {
                k: [t[i:i+block_size] for i in range(0, total_length, block_size)]
                for k, t in concatenated.items()
            }
            result["labels"] = result["input_ids"].copy()
            return result
        
        lm_dataset = tokenized.map(
            group_texts,
            batched=True,
            batch_size=5000,
            num_proc=multiprocessing.cpu_count(),
            desc="Packing",
        )
        
        # ä¿å­˜ç¼“å­˜
        lm_dataset.save_to_disk(str(lm_dataset_path))
        print_rank0(f"âœ… æ•°æ®å¤„ç†å®Œæˆ: {len(lm_dataset)} æ ·æœ¬", local_rank)
    
    # ç­‰å¾…ä¸»è¿›ç¨‹
    if dist.is_initialized():
        dist.barrier()
    
    lm_dataset = load_from_disk(str(lm_dataset_path))
    return lm_dataset


# ============================================================
# æ¨¡å‹åˆ›å»º
# ============================================================

def create_model(args, tokenizer, local_rank):
    """åˆ›å»ºå¹¶ä¼˜åŒ–æ¨¡å‹"""
    from transformers import GPT2Config, GPT2LMHeadModel
    
    print_rank0("ğŸ—ï¸ åˆ›å»ºæ¨¡å‹...", local_rank)
    
    config = GPT2Config(
        vocab_size=len(tokenizer),
        n_positions=args.n_positions,
        n_ctx=args.n_positions,
        n_embd=args.n_embd,
        n_layer=args.n_layer,
        n_head=args.n_head,
        n_inner=args.n_embd * 4,
        activation_function="gelu_new",
        resid_pdrop=0.0,
        embd_pdrop=0.0,
        attn_pdrop=0.0,
        bos_token_id=tokenizer.bos_token_id,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id,
    )
    
    # === 5090/Blackwell ä¼˜åŒ–ï¼šFlash Attention 2 ===
    if args.use_flash_attn:
        try:
            # å°è¯•ä½¿ç”¨ Flash Attention 2
            config._attn_implementation = "flash_attention_2"
            print_rank0("âœ… Flash Attention 2 å·²å¯ç”¨", local_rank)
        except Exception:
            # å›é€€åˆ° SDPA
            config._attn_implementation = "sdpa"
            print_rank0("âš ï¸ Flash Attention 2 ä¸å¯ç”¨ï¼Œä½¿ç”¨ SDPA", local_rank)
    else:
        config._attn_implementation = "sdpa"
    
    model = GPT2LMHeadModel(config)
    param_count = sum(p.numel() for p in model.parameters()) / 1e6
    print_rank0(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ: {param_count:.1f}M å‚æ•°", local_rank)
    
    # Liger Kernel ä¼˜åŒ–
    if args.use_liger:
        try:
            from liger_kernel.transformers import LigerLayerNorm
            import torch.nn as nn
            
            count = 0
            for name, module in list(model.named_modules()):
                if isinstance(module, nn.LayerNorm):
                    parent_name = '.'.join(name.split('.')[:-1])
                    child_name = name.split('.')[-1]
                    parent = model.get_submodule(parent_name) if parent_name else model
                    liger_ln = LigerLayerNorm(module.normalized_shape, eps=module.eps)
                    liger_ln.weight = module.weight
                    if module.bias is not None:
                        liger_ln.bias = module.bias
                    setattr(parent, child_name, liger_ln)
                    count += 1
            print_rank0(f"âœ… Liger LayerNorm: æ›¿æ¢äº† {count} ä¸ª", local_rank)
        except ImportError:
            print_rank0("âš ï¸ liger-kernel æœªå®‰è£…ï¼Œè·³è¿‡ä¼˜åŒ–", local_rank)
    
    # Gradient Checkpointing
    model.gradient_checkpointing_enable()
    print_rank0("âœ… Gradient Checkpointing å·²å¯ç”¨", local_rank)
    
    # === 5090/Blackwell ä¼˜åŒ–ï¼štorch.compile max-autotune ===
    if args.use_compile:
        try:
            # max-autotune æ¨¡å¼ä¼šèŠ±æ›´å¤šæ—¶é—´ç¼–è¯‘ï¼Œä½†è¿è¡Œæ›´å¿«
            model = torch.compile(model, mode=args.compile_mode)
            print_rank0(f"âœ… torch.compile å·²å¯ç”¨ (mode={args.compile_mode})", local_rank)
        except Exception as e:
            print_rank0(f"âš ï¸ torch.compile å¤±è´¥: {e}", local_rank)
    
    return model


# ============================================================
# è®­ç»ƒ
# ============================================================

def train(args):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # === 1. ç‰ˆæœ¬å…¼å®¹æ€§æ£€æŸ¥ ===
    check_environment()
    
    # åˆ†å¸ƒå¼è®¾ç½®
    local_rank, world_size, is_distributed = setup_distributed()
    device = torch.device(f"cuda:{local_rank}")
    
    print_rank0("=" * 60, local_rank)
    print_rank0("ğŸš€ GPT-2 Chinese Pretraining (AutoDL 5090)", local_rank)
    print_rank0("=" * 60, local_rank)
    print_rank0(f"   GPU: {torch.cuda.get_device_name(local_rank)}", local_rank)
    print_rank0(f"   åˆ†å¸ƒå¼: {is_distributed} (world_size={world_size})", local_rank)
    print_rank0(f"   å·¥ä½œç›®å½•: {args.work_dir}", local_rank)
    
    # åˆ›å»ºå·¥ä½œç›®å½•
    Path(args.work_dir).mkdir(parents=True, exist_ok=True)
    Path(args.cache_dir).mkdir(parents=True, exist_ok=True)
    
    # HuggingFace ç™»å½•
    if args.hf_token:
        from huggingface_hub import login
        login(token=args.hf_token)
    elif os.environ.get("HF_TOKEN"):
        args.hf_token = os.environ["HF_TOKEN"]
        from huggingface_hub import login
        login(token=args.hf_token)
    
    # åŠ è½½æ•°æ®
    dataset = load_datasets(args, local_rank)
    
    # åˆ†è¯å™¨
    tokenizer = train_or_load_tokenizer(args, dataset, local_rank)
    
    # æ•°æ®é¢„å¤„ç†
    lm_dataset = prepare_dataset(args, dataset, tokenizer, local_rank)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model(args, tokenizer, local_rank)
    model = model.to(device)
    
    # DDP åŒ…è£…
    if is_distributed:
        model = DDP(model, device_ids=[local_rank])
        print_rank0("âœ… DDP å·²å¯ç”¨", local_rank)
    
    # è®­ç»ƒå‚æ•°
    from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling
    
    effective_batch = args.batch_size * args.gradient_accumulation * world_size
    print_rank0(f"\nğŸ“Š è®­ç»ƒé…ç½®:", local_rank)
    print_rank0(f"   Batch/GPU: {args.batch_size}", local_rank)
    print_rank0(f"   æœ‰æ•ˆ Batch: {effective_batch}", local_rank)
    print_rank0(f"   Epochs: {args.num_epochs}", local_rank)
    
    training_args = TrainingArguments(
        output_dir=str(Path(args.work_dir) / "checkpoints"),
        overwrite_output_dir=True,
        
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation,
        
        learning_rate=args.learning_rate,
        warmup_steps=args.warmup_steps,
        lr_scheduler_type="cosine",
        
        optim="adamw_bnb_8bit" if args.use_8bit_adam else "adamw_torch",
        weight_decay=0.1,
        max_grad_norm=1.0,
        
        # === 5090/Blackwell ä¼˜åŒ–ï¼šä½¿ç”¨ BF16 ===
        # BF16 æ¯” FP16 æ•°å€¼èŒƒå›´æ›´å¤§ï¼Œè®­ç»ƒæ›´ç¨³å®š
        fp16=not args.use_bf16,
        bf16=args.use_bf16 and torch.cuda.is_bf16_supported(),
        gradient_checkpointing=False,  # å·²æ‰‹åŠ¨å¯ç”¨
        
        eval_strategy="steps",
        eval_steps=500,
        save_strategy="steps",
        save_steps=500,
        save_total_limit=3,
        prediction_loss_only=True,
        
        logging_steps=10,
        logging_first_step=True,
        report_to="none",
        
        push_to_hub=args.push_to_hub and is_main_process(local_rank),
        hub_model_id=args.hub_model_id,
        hub_strategy="checkpoint",
        
        dataloader_num_workers=4,
        dataloader_pin_memory=True,
        seed=args.seed,
        
        # DDP è®¾ç½®
        ddp_find_unused_parameters=False,
        local_rank=local_rank if is_distributed else -1,
    )
    
    # Data Collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )
    
    # === Callback: ç”Ÿæˆæµ‹è¯•ï¼ˆè¯„ä¼°/ä¿å­˜æ—¶æµ‹è¯• promptï¼‰===
    from transformers import TrainerCallback
    
    class GenerationCallback(TrainerCallback):
        """åœ¨è¯„ä¼°/ä¿å­˜æ—¶æµ‹è¯•ç”Ÿæˆè´¨é‡"""
        def __init__(self, tokenizer, prompts=None):
            self.tokenizer = tokenizer
            self.prompts = prompts or ["ä¸­å›½çš„å†å²", "äººå·¥æ™ºèƒ½æ˜¯", "ä»Šå¤©å¤©æ°”"]
        
        def on_evaluate(self, args, state, control, model, **kwargs):
            print("\n" + "=" * 50)
            print(f"ğŸ“ Step {state.global_step} - ç”Ÿæˆæ ·æœ¬æµ‹è¯•:")
            print("=" * 50)
            
            # å¤„ç† DDP åŒ…è£…
            eval_model = model.module if hasattr(model, 'module') else model
            eval_model.eval()
            device = next(eval_model.parameters()).device
            
            for prompt in self.prompts:
                try:
                    inputs = self.tokenizer(prompt, return_tensors="pt").to(device)
                    with torch.no_grad():
                        outputs = eval_model.generate(
                            **inputs, max_new_tokens=50,
                            do_sample=True, temperature=0.8, top_k=50,
                            pad_token_id=self.tokenizer.pad_token_id,
                        )
                    generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                    print(f"   [{prompt}] â†’ {generated[:80]}...")
                except Exception as e:
                    print(f"   [{prompt}] â†’ ç”Ÿæˆå¤±è´¥: {e}")
            print("=" * 50 + "\n")
    
    # === Callback: å‰10æ­¥è¯¦ç»†æ—¥å¿— ===
    class DetailedLoggingCallback(TrainerCallback):
        """å‰10æ­¥è¯¦ç»†æ—¥å¿—ï¼Œä¹‹åæ¯100æ­¥æ‰“å°ä¸€æ¬¡"""
        def on_log(self, args, state, control, logs=None, **kwargs):
            if logs:
                step = state.global_step
                loss = logs.get("loss", logs.get("eval_loss", None))
                lr = logs.get("learning_rate", 0)
                
                # å‰10æ­¥æˆ–æ¯100æ­¥æ‰“å°è¯¦ç»†ä¿¡æ¯
                if step <= 10 or step % 100 == 0:
                    if loss is not None:
                        print(f"ğŸ“Š Step {step}: loss={loss:.4f}, lr={lr:.2e}")
    
    # åˆ›å»ºå›è°ƒ
    generation_callback = GenerationCallback(tokenizer) if is_main_process(local_rank) else None
    logging_callback = DetailedLoggingCallback()
    
    callbacks = [logging_callback]
    if generation_callback:
        callbacks.append(generation_callback)
    
    # åˆ›å»º Trainer
    # å¦‚æœæ˜¯ DDPï¼Œéœ€è¦è§£åŒ…æ¨¡å‹
    train_model = model.module if is_distributed else model
    
    trainer = Trainer(
        model=train_model,
        args=training_args,
        train_dataset=lm_dataset,
        eval_dataset=lm_dataset.select(range(min(1000, len(lm_dataset)))),
        tokenizer=tokenizer,
        data_collator=data_collator,
        callbacks=callbacks,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print_rank0("\n" + "=" * 60, local_rank)
    print_rank0("ğŸ¯ å¼€å§‹è®­ç»ƒ...", local_rank)
    print_rank0("=" * 60, local_rank)
    
    try:
        trainer.train(resume_from_checkpoint=args.resume)
        print_rank0("\nâœ… è®­ç»ƒå®Œæˆ!", local_rank)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        if is_main_process(local_rank):
            final_path = Path(args.work_dir) / "final_model"
            trainer.save_model(str(final_path))
            tokenizer.save_pretrained(str(final_path))
            
            # ä¸Šä¼ åˆ° Hub
            if args.push_to_hub and args.hf_token:
                print_rank0("ğŸ“¤ ä¸Šä¼ åˆ° HuggingFace Hub...", local_rank)
                from huggingface_hub import HfApi
                api = HfApi(token=args.hf_token)
                user = api.whoami()["name"]
                repo_id = args.hub_model_id or f"{user}/gpt2-chinese-mini"
                
                api.upload_folder(
                    folder_path=str(final_path),
                    repo_id=repo_id,
                    commit_message="Training complete",
                )
                print_rank0(f"ğŸ‰ æ¨¡å‹å·²ä¸Šä¼ è‡³: https://huggingface.co/{repo_id}", local_rank)
                
    except KeyboardInterrupt:
        print_rank0("\nâš ï¸ è®­ç»ƒè¢«ä¸­æ–­ï¼Œä¿å­˜ checkpoint...", local_rank)
        trainer.save_model()
    
    finally:
        cleanup_distributed()


# ============================================================
# ä¸»å…¥å£
# ============================================================

if __name__ == "__main__":
    args = parse_args()
    train(args)
