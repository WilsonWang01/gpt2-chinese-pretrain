# GPT-2 ä¸­æ–‡é¢„è®­ç»ƒè¸©å‘æŒ‡å—

> æœ¬æ–‡æ¡£è®°å½•äº† A100/5090 è®­ç»ƒè„šæœ¬å¼€å‘è¿‡ç¨‹ä¸­é‡åˆ°çš„æ‰€æœ‰é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆï¼Œé¿å…åç»­è¸©å‘ã€‚

---

## ğŸ”´ ä¸¥é‡é—®é¢˜ (ä¼šå¯¼è‡´å´©æºƒ)

### 1. DDP + torch.compile + Gradient Checkpointing å†²çª

**æŠ¥é”™ä¿¡æ¯**:
```
RuntimeError: Parameter ... has been marked as ready twice
RuntimeError: expect_autograd_hooks_ INTERNAL ASSERT FAILED
```

**åŸå› **: ä¸‰è€…åŒæ—¶å¯ç”¨ä¼šå¯¼è‡´ PyTorch å†…éƒ¨çŠ¶æ€å†²çª

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ–¹æ¡ˆ1: äºŒé€‰ä¸€ (æ¨è torch.compileï¼ŒA100 40GB å¤Ÿç”¨)
gradient_checkpointing=not args.use_compile,
torch_compile=args.use_compile,

# æ–¹æ¡ˆ2: ç¦ç”¨ torch.compile
gradient_checkpointing=True,
torch_compile=False,
```

---

### 2. æ‰‹åŠ¨ DDP + Trainer è‡ªåŠ¨ DDP = åŒé‡åŒ…è£…

**æŠ¥é”™ä¿¡æ¯**:
```
RuntimeError: Expected all tensors to be on the same device
AttributeError: 'DistributedDataParallel' object has no attribute 'config'
```

**åŸå› **: åŒæ—¶æ‰‹åŠ¨è°ƒç”¨ `DDP(model)` å’Œè®¾ç½® `TrainingArguments(local_rank=...)`

**è§£å†³æ–¹æ¡ˆ**:
```python
# âŒ é”™è¯¯åšæ³•
model = DDP(model, device_ids=[local_rank])
trainer = Trainer(model=model, ...)

# âœ… æ­£ç¡®åšæ³• - è®© Trainer è‡ªåŠ¨å¤„ç†
trainer = Trainer(model=model, ...)  # ç›´æ¥ä¼ å…¥ï¼Œä¸è¦æ‰‹åŠ¨ DDP
```

---

### 3. torch.compile ä¸ Checkpoint ä¿å­˜å†²çª

**æŠ¥é”™ä¿¡æ¯**:
```
KeyError: '_orig_mod.transformer.wte.weight'
RuntimeError: Error(s) in loading state_dict
```

**åŸå› **: `torch.compile` ä¼šå°†æƒé‡ååŠ ä¸Š `_orig_mod` å‰ç¼€

**è§£å†³æ–¹æ¡ˆ**:
```python
# TrainingArguments è‡ªåŠ¨å¤„ç†
training_args = TrainingArguments(
    torch_compile=True,  # è®© Trainer ç®¡ç† compile
    ...
)
```

---

### 4. bitsandbytes CUDA 12 å…¼å®¹æ€§

**æŠ¥é”™ä¿¡æ¯**:
```
The installed version of bitsandbytes was compiled without GPU support
libbitsandbytes_cuda124.so: cannot open shared object file
```

**åŸå› **: bitsandbytes äºŒè¿›åˆ¶ç‰ˆæœ¬ä¸ CUDA 12.x ä¸å®Œå…¨å…¼å®¹

**è§£å†³æ–¹æ¡ˆ**:
```python
def get_optimizer_name(use_8bit_adam):
    if not use_8bit_adam:
        return "adamw_torch_fused"
    try:
        import bitsandbytes as bnb
        test_param = torch.zeros(1, device='cuda', requires_grad=True)
        bnb.optim.Adam8bit([test_param])  # æµ‹è¯•æ˜¯å¦çœŸæ­£æ”¯æŒ
        return "adamw_bnb_8bit"
    except Exception:
        return "adamw_torch_fused"  # Fallback
```

---

### 5. resume æ—  Checkpoint æ—¶å´©æºƒ

**æŠ¥é”™ä¿¡æ¯**:
```
ValueError: No valid checkpoint found in output directory
```

**è§£å†³æ–¹æ¡ˆ**:
```python
resume_path = None
if args.resume:
    checkpoint_dir = Path(args.work_dir) / "checkpoints"
    checkpoints = list(checkpoint_dir.glob("checkpoint-*"))
    if checkpoints:
        resume_path = True
    else:
        print("âš ï¸ æœªæ‰¾åˆ° checkpointï¼Œä»å¤´å¼€å§‹")
trainer.train(resume_from_checkpoint=resume_path)
```

---

## ğŸŸ  ä¸­ç­‰é—®é¢˜ (æ€§èƒ½ä¸‹é™)

### 6. DataLoader num_workers è¿‡ä½

**ç°è±¡**: GPU åˆ©ç”¨ç‡ 50-70%ï¼Œ`nvidia-smi` æ˜¾ç¤º GPU ç»å¸¸ç©ºé—²

**è§£å†³æ–¹æ¡ˆ**:
```python
dataloader_num_workers=min(8, multiprocessing.cpu_count()),
dataloader_persistent_workers=True,  # é¿å…æ¯ epoch é‡å»º
dataloader_pin_memory=True,
```

---

### 7. check_environment è¿”å› None

**é—®é¢˜**: DDP æ—¶éä¸»è¿›ç¨‹ Flash Attention å¤±æ•ˆ

```python
# âŒ é”™è¯¯
def check_environment(local_rank=0):
    if local_rank != 0:
        return  # è¿”å› None

# âœ… æ­£ç¡®
def check_environment(local_rank=0):
    if local_rank != 0:
        return False  # æ˜ç¡®è¿”å› False
```

---

### 8. Hub ä»“åº“ä¸å­˜åœ¨æ—¶ä¸Šä¼ å¤±è´¥

**æŠ¥é”™ä¿¡æ¯**:
```
huggingface_hub.utils._errors.RepositoryNotFoundError
```

**è§£å†³æ–¹æ¡ˆ**:
```python
api.create_repo(repo_id=repo_id, exist_ok=True, private=False)  # å…ˆåˆ›å»º
api.upload_folder(folder_path=..., repo_id=repo_id, ...)
```

---

### 9. SentencePiece è®­ç»ƒæ…¢

**ç°è±¡**: åˆ†è¯å™¨è®­ç»ƒ 10+ åˆ†é’Ÿ

**è§£å†³æ–¹æ¡ˆ**:
```python
spm.SentencePieceTrainer.train(
    input_sentence_size=200000,  # é™åˆ¶æ ·æœ¬æ•°
    shuffle_input_sentence=True,
    num_threads=multiprocessing.cpu_count(),
    ...
)
```

---

### 10. map batch_size è¿‡å¤§å¯¼è‡´ OOM

**æŠ¥é”™ä¿¡æ¯**:
```
Killed (signal 9)
MemoryError
```

**è§£å†³æ–¹æ¡ˆ**:
```python
dataset.map(
    tokenize_fn,
    batch_size=2000,  # ä» 5000 é™åˆ° 2000
    num_proc=min(8, cpu_count()),
)
```

---

## ğŸŸ¡ æ³¨æ„äº‹é¡¹

### 11. Flash Attention éœ€è¦ Ampere+ æ¶æ„

```python
if torch.cuda.get_device_capability()[0] >= 8:  # SM 8.0+
    # æ”¯æŒ Flash Attention (A100, A10, RTX 30xx, 40xx, 50xx)
else:
    # ä¸æ”¯æŒ (V100, T4, P100)
```

---

### 12. TF32 åªæœ‰ Ampere+ æ”¯æŒ

```python
# A100/A800/H100/5090 å¯ç”¨ï¼ŒV100/T4 ä¸å¯ç”¨
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
```

---

### 13. ç¯å¢ƒå˜é‡è®¾ç½®é¡ºåº

```python
# å¿…é¡»åœ¨ä»»ä½• HuggingFace å¯¼å…¥ä¹‹å‰è®¾ç½®
os.environ.setdefault("HF_HOME", "/root/autodl-tmp/cache")
os.environ.setdefault("TRANSFORMERS_CACHE", "/root/autodl-tmp/cache")

# ç„¶åæ‰èƒ½ import
from transformers import ...
```

---

### 14. å®Œæ•´éšæœºç§å­è®¾ç½®

```python
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
```

---

## ğŸ“Š GPU æ¨èé…ç½®

| GPU | batch_size | compile_mode | ç‰¹æ®Šä¼˜åŒ– |
|-----|-----------|--------------|---------|
| A100-40GB | 48 | reduce-overhead | TF32 |
| A100-80GB | 64 | reduce-overhead | TF32 |
| RTX 5090 | 48 | max-autotune | - |
| RTX 4090 | 32 | max-autotune | - |
| T4 (Kaggle) | 8 | ä¸æ¨è | DataParallel |

---

## ğŸ”§ è°ƒè¯•å‘½ä»¤

```bash
# æŸ¥çœ‹ GPU çŠ¶æ€
watch -n 1 nvidia-smi

# æŸ¥çœ‹ CUDA ç‰ˆæœ¬
nvcc --version
python -c "import torch; print(torch.version.cuda)"

# æµ‹è¯• bitsandbytes
python -c "import bitsandbytes; print(bitsandbytes.__version__)"

# æµ‹è¯• Flash Attention
python -c "import flash_attn; print(flash_attn.__version__)"

# æŸ¥çœ‹ GPU æ¶æ„
python -c "import torch; print(torch.cuda.get_device_capability())"
```

---

## ğŸ“ ç‰ˆæœ¬è¦æ±‚

| ä¾èµ– | æœ€ä½ç‰ˆæœ¬ | æ¨èç‰ˆæœ¬ |
|-----|---------|---------|
| Python | 3.10 | 3.11 |
| PyTorch | 2.0 | 2.2+ |
| CUDA | 11.8 (A100) | 12.1+ |
| Transformers | 4.36 | 4.40+ |
| flash-attn | 2.0 | 2.5+ |

---

*æœ€åæ›´æ–°: 2026-02-03*

---

## ğŸ”´ æ–°å¢ä¸¥é‡é—®é¢˜ (2026-02-03 è®­ç»ƒè¡¥å……)

### 15. GenerationCallback ä¸­ datetime æœªå¯¼å…¥

**æŠ¥é”™ä¿¡æ¯**:
```
NameError: name 'datetime' is not defined. Did you forget to import 'datetime'?
```

**åŸå› **: åœ¨ callback ç±»ä¸­ä½¿ç”¨ `datetime.now()` ä½†å¿˜è®°åœ¨è„šæœ¬é¡¶éƒ¨å¯¼å…¥

**è§£å†³æ–¹æ¡ˆ**:
```python
# åœ¨è„šæœ¬é¡¶éƒ¨æ·»åŠ 
from datetime import datetime
```

---

### 16. Tokenizer è¯è¡¨å¤§å°é”™è¯¯ (unk_id é—®é¢˜) - "Final Boss"

**æŠ¥é”™ä¿¡æ¯**:
```
ValueError: unk_id 0 is out of range (vocab_size = 3)
# æˆ–
RuntimeError: Vocabulary size mismatch: config=32000, actual=3
```

**åŸå› **: HuggingFace çš„ `LlamaTokenizerFast` / `AutoTokenizer` åŠ è½½ SentencePiece æ¨¡å‹æ—¶è§£æé”™è¯¯ï¼Œè¿”å›é”™è¯¯çš„è¯è¡¨å¤§å°

**è§£å†³æ–¹æ¡ˆ** (æ‰‹åŠ¨é‡å»º tokenizer.json):
```python
from tokenizers import Tokenizer, decoders, pre_tokenizers
from tokenizers.models import Unigram
from transformers import PreTrainedTokenizerFast

# 1. è¯»å– SP è¯è¡¨æ–‡ä»¶
vocab_path = Path(work_dir) / "chinese_sp.vocab"
vocab_list = []
with open(vocab_path, "r", encoding="utf-8") as f:
    for line in f:
        parts = line.strip().split("\t")
        if len(parts) >= 2:
            vocab_list.append((parts[0], float(parts[1])))

# 2. ç”¨ tokenizers åº“é‡å»º
tokenizer_obj = Tokenizer(Unigram(vocab_list))
tokenizer_obj.decoder = decoders.Metaspace()
tokenizer_obj.pre_tokenizer = pre_tokenizers.Metaspace()
tokenizer_obj.save(str(tokenizer_dir / "tokenizer.json"))

# 3. ç”¨ PreTrainedTokenizerFast åŠ è½½
tokenizer = PreTrainedTokenizerFast(
    tokenizer_file=str(tokenizer_dir / "tokenizer.json"),
    bos_token="<s>", eos_token="</s>",
    unk_token="<unk>", pad_token="<pad>",
)
```

---

### 17. Checkpoint åªä¿ç•™æœ€å 3 ä¸ªï¼Œæ— æ³•å¯¹æ¯”æ—©æœŸæ•ˆæœ

**ç°è±¡**: è®­ç»ƒç»“æŸååªæœ‰ checkpoint-11000, 11500, 11838ï¼Œæ— æ³•å¯¹æ¯” step 100/500/1000 æ—¶çš„éšæœºç”Ÿæˆæ•ˆæœ

**åŸå› **: `save_total_limit=3` å¯¼è‡´æ—©æœŸ checkpoint è¢«è‡ªåŠ¨åˆ é™¤

**è§£å†³æ–¹æ¡ˆ**:
```python
# å¢åŠ ä¿ç•™æ•°é‡
training_args = TrainingArguments(
    save_steps=500,
    save_total_limit=15,  # ä¿ç•™æ›´å¤š checkpoint
    ...
)
```

---

### 18. ç”Ÿæˆæ ·æœ¬åªæ‰“å°åˆ°æ§åˆ¶å°ï¼Œè®­ç»ƒåæ— æ³•æ‰¾å›

**é—®é¢˜**: `GenerationCallback` åª `print()` ç”Ÿæˆç»“æœï¼Œè®­ç»ƒç»“æŸåæ— æ³•æŸ¥çœ‹å†å²æ ·æœ¬

**è§£å†³æ–¹æ¡ˆ** (ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶):
```python
class GenerationCallback(TrainerCallback):
    def __init__(self, tokenizer, work_dir, prompts=None):
        self.log_file = Path(work_dir) / "generation_samples.log"
        # åˆå§‹åŒ–æ—¥å¿—
        with open(self.log_file, "w", encoding="utf-8") as f:
            f.write(f"# åˆ›å»ºæ—¶é—´: {datetime.now()}\n")
    
    def on_evaluate(self, args, state, control, model, **kwargs):
        # ... ç”Ÿæˆé€»è¾‘ ...
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(f"\n{'='*70}\n")
            f.write(f"Step {step} | Loss: {loss}\n")
            for prompt, result in results:
                f.write(f"[{prompt}] â†’ {result}\n")
```

---

### 19. argparse å‚æ•°åæ ¼å¼é”™è¯¯

**æŠ¥é”™ä¿¡æ¯**:
```
error: unrecognized arguments: --work-dir --batch-size
```

**åŸå› **: Python argparse å®šä¹‰æ—¶ç”¨çš„ `work_dir`ï¼Œè°ƒç”¨æ—¶ç”¨ `work-dir` (è¿å­—ç¬¦ vs ä¸‹åˆ’çº¿)

**è§£å†³æ–¹æ¡ˆ**:
```bash
# âŒ é”™è¯¯
python a100_train.py --work-dir gpt2-chinese --batch-size 48

# âœ… æ­£ç¡® (ç”¨ä¸‹åˆ’çº¿)
python a100_train.py --work_dir gpt2-chinese --batch_size 48
```

**é¢„é˜²**: åœ¨ argparse å®šä¹‰æ—¶åŒæ—¶æ·»åŠ ä¸¤ç§æ ¼å¼:
```python
parser.add_argument("--work_dir", "--work-dir", type=str, default="gpt2-chinese")
```

---

### 20. eval_loss ä¸æ˜¾ç¤º

**ç°è±¡**: è®­ç»ƒæ—¥å¿—åªæœ‰ `train_loss`ï¼Œæ²¡æœ‰ `eval_loss`

**åŸå› **: è®¾ç½®äº† `prediction_loss_only=True`

**è§£å†³æ–¹æ¡ˆ**:
```python
training_args = TrainingArguments(
    eval_strategy="steps",
    eval_steps=500,
    # prediction_loss_only=False,  # æ³¨é‡Šæ‰æˆ–è®¾ä¸º False
    ...
)
```

---

## ğŸŸ  æ–°å¢ä¸­ç­‰é—®é¢˜

### 21. SCP æ–‡ä»¶ä¼ è¾“éœ€è¦å¯†ç äº¤äº’

**ç°è±¡**: `scp` å‘½ä»¤é˜»å¡ç­‰å¾…å¯†ç è¾“å…¥

**è§£å†³æ–¹æ¡ˆ**:
1. è®¾ç½® SSH å…å¯†ç™»å½•
2. æˆ–ä½¿ç”¨ Base64 ç¼–ç é€šè¿‡ SSH ä¼ è¾“:
```bash
# æœåŠ¡å™¨ç«¯: ç¼–ç å‹ç¼©
tar czf - files/ | base64 > transfer.b64

# æœ¬åœ°: è§£ç 
cat transfer.b64 | base64 -d | tar xzf -
```

---

### 22. Generation æ—¶ results å˜é‡æœªå®šä¹‰

**æŠ¥é”™ä¿¡æ¯**:
```
NameError: name 'results' is not defined
```

**åŸå› **: åœ¨å¾ªç¯å‰å¿˜è®°åˆå§‹åŒ– `results = []`

**è§£å†³æ–¹æ¡ˆ**:
```python
def on_evaluate(...):
    results = [header]  # åˆå§‹åŒ–åˆ—è¡¨
    for prompt in self.prompts:
        # ...
        results.append(line)
```

---

## ğŸ“‹ è®­ç»ƒæ—¥å¿—ä¿å­˜æ£€æŸ¥æ¸…å•

è¿è¡Œè®­ç»ƒå‰ç¡®ä¿ä»¥ä¸‹é¡¹ç›®å·²é…ç½®:

- [ ] `save_total_limit >= 10` (ä¿ç•™è¶³å¤Ÿå¤šçš„ checkpoint)
- [ ] `GenerationCallback` ä¿å­˜åˆ°æ–‡ä»¶è€Œéåª print
- [ ] `logging_steps=10` (é¢‘ç¹è®°å½• loss)
- [ ] `eval_strategy="steps"` + `eval_steps` å·²è®¾ç½®
- [ ] `report_to="tensorboard"` æˆ–å…¶ä»–æ—¥å¿—å·¥å…·
- [ ] è®­ç»ƒç»“æŸåä¸‹è½½ `trainer_state.json` + `generation_samples.log`

---

## ğŸ”§ è¿œç¨‹æœåŠ¡å™¨è®­ç»ƒå®Œæ•´æµç¨‹

```bash
# 1. ä¸Šä¼ è„šæœ¬
scp -P 35036 a100_train.py root@server:/root/autodl-tmp/

# 2. SSH è¿æ¥
ssh -p 35036 root@server

# 3. å¯åŠ¨è®­ç»ƒ (ç”¨ nohup é˜²æ­¢æ–­è¿)
cd /root/autodl-tmp
nohup python3 a100_train.py --work_dir gpt2-chinese --batch_size 48 --use_bf16 > train.log 2>&1 &

# 4. æŸ¥çœ‹è¿›åº¦
tail -f train.log

# 5. è®­ç»ƒå®Œæˆåä¸‹è½½
scp -P 35036 root@server:/root/autodl-tmp/gpt2-chinese/generation_samples.log ./
scp -P 35036 -r root@server:/root/autodl-tmp/gpt2-chinese/checkpoints/checkpoint-*/trainer_state.json ./
```

---

*æœ€åæ›´æ–°: 2026-02-03 (è¡¥å……è®­ç»ƒæ—¥å¿—ä¿å­˜ç»éªŒ)*
