# Kaggle Mini GPT-2 ä¸­æ–‡é¢„è®­ç»ƒ - å¤šæ•°æ®é›†ç‰ˆæœ¬
# ===================================================
# ä½¿ç”¨ ç»´åŸºç™¾ç§‘ + çŸ¥ä¹ åˆå¹¶æ•°æ®é›† (~625M tokens)
# è®¾ç½®: Accelerator = GPU T4 x2, Internet = On
# ===================================================

# ================== Cell 1: ç¯å¢ƒè®¾ç½® ==================
print("=" * 60)
print("ğŸ”§ ç¬¬ä¸€é˜¶æ®µï¼šç¯å¢ƒé…ç½®")
print("=" * 60)

import subprocess
subprocess.run(["nvidia-smi"], check=True)

subprocess.run([
    "pip", "install", "-q",
    "transformers", "datasets", "accelerate",
    "huggingface_hub", "sentencepiece", "tokenizers",
    "bitsandbytes",      # 8-bit ä¼˜åŒ–å™¨
    "liger-kernel",      # Triton LayerNorm åŠ é€Ÿ
], check=True)

import torch
print(f"\nâœ… PyTorch: {torch.__version__}")
print(f"âœ… CUDA: {torch.cuda.is_available()}")
print(f"âœ… GPU æ•°é‡: {torch.cuda.device_count()}")

# ================== Cell 2: HuggingFace ç™»å½• ==================
from huggingface_hub import login

# ä» Kaggle Secrets è·å– Token
try:
    from kaggle_secrets import UserSecretsClient
    user_secrets = UserSecretsClient()
    hf_token = user_secrets.get_secret("HF_TOKEN")
    login(token=hf_token)
    print("âœ… ä½¿ç”¨ Kaggle Secret ç™»å½•æˆåŠŸ")
except Exception as e:
    print(f"âš ï¸ Secret è·å–å¤±è´¥: {e}")
    from huggingface_hub import notebook_login
    notebook_login()

from huggingface_hub import whoami
try:
    HF_USERNAME = whoami()["name"]
    print(f"âœ… å½“å‰ç”¨æˆ·: {HF_USERNAME}")
except:
    HF_USERNAME = "YOUR_USERNAME"
    print(f"âš ï¸ ä½¿ç”¨é»˜è®¤ç”¨æˆ·å: {HF_USERNAME}")

# ================== Cell 3: å¤šæ•°æ®é›†åŠ è½½ ==================
print("\n" + "=" * 60)
print("ğŸ“¥ ç¬¬äºŒé˜¶æ®µï¼šå¤šæ•°æ®é›†åŠ è½½")
print("=" * 60)

from datasets import load_dataset, concatenate_datasets
import os

print("ğŸ“¥ æ­£åœ¨ä¸‹è½½å¤šä¸ªæ•°æ®é›†...")

# === 1. åŠ è½½ç»´åŸºç™¾ç§‘ï¼ˆå®Œæ•´ï¼‰===
print("   [1/2] åŠ è½½ç»´åŸºç™¾ç§‘...")
wiki = load_dataset(
    "pleisto/wikipedia-cn-20230720-filtered",
    split="train",
)
print(f"   âœ… ç»´åŸºç™¾ç§‘: {len(wiki)} æ¡")

# === 2. åŠ è½½çŸ¥ä¹é«˜èµå›ç­” ===
print("   [2/2] åŠ è½½çŸ¥ä¹...")
zhihu = load_dataset(
    "wangrui6/Zhihu-KOL",
    split="train",
)
print(f"   âœ… çŸ¥ä¹: {len(zhihu)} æ¡")

# === 3. ç»Ÿä¸€å­—æ®µåå¹¶åˆå¹¶ ===
def process_wiki(example):
    return {"text": example["completion"]}

def process_zhihu(example):
    text = f"{example['INSTRUCTION']}\n{example['RESPONSE']}"
    return {"text": text}

print("ğŸ”„ æ­£åœ¨å¤„ç†æ•°æ®æ ¼å¼...")
wiki_processed = wiki.map(process_wiki, remove_columns=wiki.column_names, num_proc=4)
zhihu_processed = zhihu.map(process_zhihu, remove_columns=zhihu.column_names, num_proc=4)

dataset = concatenate_datasets([wiki_processed, zhihu_processed])
dataset = dataset.shuffle(seed=42)

print(f"\nâœ… æ•°æ®é›†åˆå¹¶å®Œæˆ!")
print(f"   ç»´åŸºç™¾ç§‘: {len(wiki)} æ¡")
print(f"   çŸ¥ä¹: {len(zhihu)} æ¡")
print(f"   åˆè®¡: {len(dataset)} æ¡")

# === 4. å¯¼å‡ºçº¯æ–‡æœ¬ ===
CORPUS_FILE = "/kaggle/working/combined_corpus.txt"
print("ğŸ“ æ­£åœ¨å¯¼å‡ºçº¯æ–‡æœ¬...")
# === ä¼˜åŒ–ï¼šé‡‡æ ·å¯¼å‡ºè¯­æ–™ï¼ˆä»…ç”¨äºåˆ†è¯å™¨è®­ç»ƒï¼‰===
SAMPLE_SIZE = 500000  # 50 ä¸‡å¥è¶³å¤Ÿè®­ç»ƒ 32K è¯è¡¨

# æ‰“ä¹±å¹¶é‡‡æ ·
import random
random.seed(42)
indices = list(range(len(dataset)))
random.shuffle(indices)
sampled_indices = indices[:SAMPLE_SIZE]

# æ‰¹é‡å†™å…¥ï¼ŒåŠ é€Ÿ I/O
print(f"ğŸ“ é‡‡æ · {SAMPLE_SIZE} å¥ç”¨äºåˆ†è¯å™¨è®­ç»ƒ...")
batch_size = 10000
with open(CORPUS_FILE, "w", encoding="utf-8") as f:
    batch = []
    count = 0
    for idx in sampled_indices:
        text = dataset[idx]["text"].strip()
        if len(text) > 50:
            batch.append(text)
            count += 1
            if len(batch) >= batch_size:
                f.write("\n".join(batch) + "\n")
                batch = []
    if batch:
        f.write("\n".join(batch) + "\n")

file_size_mb = os.path.getsize(CORPUS_FILE) / (1024 * 1024)
print(f"âœ… è¯­æ–™å¯¼å‡ºå®Œæˆ: {file_size_mb:.1f} MB ({count} å¥)")

# ================== Cell 4: åˆ†è¯å™¨è®­ç»ƒ ==================
print("\n" + "=" * 60)
print("ğŸ”¤ ç¬¬ä¸‰é˜¶æ®µï¼šåˆ†è¯å™¨è®­ç»ƒ")
print("=" * 60)

import sentencepiece as spm

MODEL_PREFIX = "/kaggle/working/chinese_sp"
VOCAB_SIZE = 32000

spm.SentencePieceTrainer.train(
    input=CORPUS_FILE,
    model_prefix=MODEL_PREFIX,
    vocab_size=VOCAB_SIZE,
    model_type="unigram",
    character_coverage=0.9995,
    
    # === ä¼˜åŒ–åçš„é‡‡æ ·å‚æ•° ===
    input_sentence_size=500000,       # 50 ä¸‡å¥ï¼ˆå·²é¢„é‡‡æ ·ï¼Œæ— éœ€æ›´å¤šï¼‰
    shuffle_input_sentence=True,
    max_sentence_length=2048,         # å‡å°‘å•å¥é•¿åº¦é™åˆ¶
    
    pad_id=0, unk_id=1, bos_id=2, eos_id=3,
    pad_piece="<pad>", unk_piece="<unk>",
    bos_piece="<s>", eos_piece="</s>",
    num_threads=os.cpu_count(),
    normalization_rule_name="nmt_nfkc_cf",
    split_by_unicode_script=True,
    split_by_number=True,
)
print(f"âœ… åˆ†è¯å™¨è®­ç»ƒå®Œæˆ")

# è½¬æ¢ä¸º HuggingFace æ ¼å¼
from transformers import LlamaTokenizerFast

TOKENIZER_DIR = "/kaggle/working/tokenizer"
tokenizer = LlamaTokenizerFast(
    vocab_file=f"{MODEL_PREFIX}.model",
    bos_token="<s>", eos_token="</s>",
    unk_token="<unk>", pad_token="<pad>",
    add_bos_token=False, add_eos_token=True,
)
tokenizer.save_pretrained(TOKENIZER_DIR)
print(f"âœ… è¯è¡¨å¤§å°: {len(tokenizer)}")

# ================== Cell 5: æ¨¡å‹åˆå§‹åŒ– ==================
print("\n" + "=" * 60)
print("ğŸ§  ç¬¬å››é˜¶æ®µï¼šæ¨¡å‹åˆå§‹åŒ–")
print("=" * 60)

from transformers import GPT2Config, GPT2LMHeadModel, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)

config = GPT2Config(
    vocab_size=len(tokenizer),
    # === ä¼˜åŒ–åçš„æ¨¡å‹é…ç½® ===
    n_positions=1024, n_ctx=1024,  # å¢å¤§ contextï¼ˆæ˜¾å­˜ä¼˜åŒ–åå¯æ”¯æŒï¼‰
    n_embd=768, n_layer=6, n_head=12, n_inner=3072,
    activation_function="gelu_new",
    # Dropout: å¤§æ•°æ®é›†å¯é€‚å½“é™ä½
    resid_pdrop=0.0, embd_pdrop=0.0, attn_pdrop=0.0,  # é¢„è®­ç»ƒé˜¶æ®µå…³é—­ dropout
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id,
)

# === ä¼˜åŒ– 0: SDPA (Scaled Dot Product Attention) ===
# T4 ä½¿ç”¨ memory-efficient (CUTLASS) åç«¯ï¼Œæä¾› 10-30% åŠ é€Ÿ
try:
    config._attn_implementation = "sdpa"
    print("âœ… å·²è®¾ç½® SDPA attentionï¼ˆMemory-Efficient åç«¯ï¼‰")
except Exception as e:
    print(f"âš ï¸ SDPA è®¾ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ attention: {e}")

model = GPT2LMHeadModel(config)
print(f"âœ… å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M")

# === ä¼˜åŒ– 1: Liger Kernel LayerNormï¼ˆTriton åŠ é€Ÿ +30%ï¼‰===
try:
    from liger_kernel.transformers import LigerLayerNorm
    import torch.nn as nn
    
    def patch_layernorm(model):
        """å°†æ‰€æœ‰ LayerNorm æ›¿æ¢ä¸º Liger Triton ä¼˜åŒ–ç‰ˆæœ¬"""
        patched_count = 0
        for name, module in list(model.named_modules()):
            if isinstance(module, nn.LayerNorm):
                parent_name = '.'.join(name.split('.')[:-1])
                child_name = name.split('.')[-1]
                parent = model.get_submodule(parent_name) if parent_name else model
                
                liger_ln = LigerLayerNorm(
                    module.normalized_shape,
                    eps=module.eps
                )
                liger_ln.weight = module.weight
                if module.bias is not None:
                    liger_ln.bias = module.bias
                
                setattr(parent, child_name, liger_ln)
                patched_count += 1
        return model, patched_count
    
    model, count = patch_layernorm(model)
    print(f"âœ… å·²æ›¿æ¢ {count} ä¸ª LayerNorm ä¸º Liger Triton ç‰ˆæœ¬ï¼ˆ+30% é€Ÿåº¦ï¼‰")
except ImportError:
    print("âš ï¸ liger-kernel æœªå®‰è£…ï¼Œè·³è¿‡ LayerNorm ä¼˜åŒ–")
except Exception as e:
    print(f"âš ï¸ Liger LayerNorm æ›¿æ¢å¤±è´¥: {e}")

# ä¿å­˜ lm_head.weight å¼•ç”¨ï¼Œä¾› FusedLinearCrossEntropy ä½¿ç”¨
# å¿…é¡»åœ¨ torch.compile ä¹‹å‰ä¿å­˜ï¼Œå¦åˆ™å¯èƒ½æ— æ³•è®¿é—®
LM_HEAD_WEIGHT = model.lm_head.weight

# === ä¼˜åŒ– 2: torch.compile åŠ é€Ÿï¼ˆPyTorch 2.0+ï¼‰===
try:
    model = torch.compile(model)
    print("âœ… å·²å¯ç”¨ torch.compileï¼ˆé¢„è®¡åŠ é€Ÿ 50-100%ï¼‰")
except Exception as e:
    print(f"âš ï¸ torch.compile ä¸å¯ç”¨: {e}")

# ================== Cell 6: æ•°æ®å¤„ç† ==================
print("\n" + "=" * 60)
print("ğŸ“¦ ç¬¬äº”é˜¶æ®µï¼šæ•°æ®å¤„ç†")
print("=" * 60)

BLOCK_SIZE = 1024  # ä¸æ¨¡å‹ n_positions åŒ¹é…

# è‡ªåŠ¨æ£€æµ‹å­—æ®µå
text_column = "text" if "text" in dataset.column_names else "completion"

def tokenize_function(examples):
    return tokenizer(examples[text_column], truncation=False, return_attention_mask=False)

tokenized = dataset.map(
    tokenize_function, batched=True, batch_size=1000,
    remove_columns=dataset.column_names, num_proc=4
)

def group_texts(examples):
    concatenated = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = (len(concatenated["input_ids"]) // BLOCK_SIZE) * BLOCK_SIZE
    result = {k: [t[i:i+BLOCK_SIZE] for i in range(0, total_length, BLOCK_SIZE)] for k, t in concatenated.items()}
    result["labels"] = result["input_ids"].copy()
    return result

lm_dataset = tokenized.map(group_texts, batched=True, batch_size=1000, num_proc=4)

# åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
split_dataset = lm_dataset.train_test_split(test_size=0.02, seed=42)
train_dataset = split_dataset["train"]
eval_dataset = split_dataset["test"]

print(f"âœ… è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
print(f"âœ… éªŒè¯é›†: {len(eval_dataset)} æ ·æœ¬")
print(f"   æ€» Token: {len(lm_dataset) * BLOCK_SIZE / 1e6:.1f}M")

# ================== Cell 7: è®­ç»ƒé…ç½® ==================
print("\n" + "=" * 60)
print("âš™ï¸ ç¬¬å…­é˜¶æ®µï¼šè®­ç»ƒé…ç½®")
print("=" * 60)

from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling

MODEL_NAME = "gpt2-chinese-mini-v2"
REPO_ID = f"{HF_USERNAME}/{MODEL_NAME}"

# === ä¼˜åŒ–åçš„è®­ç»ƒå‚æ•° ===
# åŸºäº Liger Kernel æ˜¾å­˜ä¼˜åŒ–ï¼Œå¯ä½¿ç”¨æ›´å¤§ batch size
NUM_EPOCHS = 2       # 625M tokens / 67.5M params â‰ˆ 9 tokens/paramï¼Œéœ€è¦å¤šæ¬¡éå†
BATCH_SIZE = 24      # æ˜¾å­˜ä¼˜åŒ–åå¯å¢å¤§ï¼ˆåŸ 16ï¼‰
GRAD_ACCUM = 2       # å‡å°‘ç´¯ç§¯æ­¥æ•°ï¼ŒåŠ å¿«æ›´æ–°ï¼ˆåŸ 4ï¼‰
# Effective batch size: 24 * 2 * 2 GPU = 96 samples = 98K tokens/step

total_steps = (len(train_dataset) // (BATCH_SIZE * GRAD_ACCUM * 2)) * NUM_EPOCHS

training_args = TrainingArguments(
    output_dir=f"/kaggle/working/{MODEL_NAME}",
    overwrite_output_dir=True,
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRAD_ACCUM,
    
    # å­¦ä¹ ç‡è°ƒåº¦ - é’ˆå¯¹å¤§ batch ä¼˜åŒ–
    learning_rate=3e-4,              # å¤§ batch æ—¶é€‚å½“é™ä½ LR
    warmup_steps=2000,               # å¢åŠ  warmupï¼ˆçº¦ 5% æ€»æ­¥æ•°ï¼‰
    lr_scheduler_type="cosine",
    
    # === ä¼˜åŒ–å™¨é…ç½® ===
    optim="adamw_bnb_8bit",          # 8-bit AdamWï¼Œæ˜¾å­˜ -75%
    weight_decay=0.1,                # æ ‡å‡† GPT-2 weight decay
    adam_beta1=0.9, adam_beta2=0.95, # GPT-2 æ ‡å‡†é…ç½®
    max_grad_norm=1.0,
    
    # ç²¾åº¦
    fp16=True,
    bf16=False,  # T4 ä¸æ”¯æŒ BF16
    
    # === ä¼˜åŒ– 3: æ•°æ®åŠ è½½ä¼˜åŒ– ===
    dataloader_num_workers=4,
    dataloader_pin_memory=True,           # é”é¡µå†…å­˜ï¼ŒåŠ é€Ÿä¼ è¾“
    dataloader_prefetch_factor=4,         # é¢„å– 4 ä¸ª batch
    dataloader_persistent_workers=True,   # æŒä¹…åŒ– workerï¼Œé¿å…é‡å¯å¼€é”€
    
    # Checkpoint
    eval_strategy="steps",
    eval_steps=500,
    save_strategy="steps",
    save_steps=500,
    save_total_limit=3,
    load_best_model_at_end=True,
    metric_for_best_model="loss",
    greater_is_better=False,
    
    # æ—¥å¿—
    logging_steps=100,
    logging_first_step=True,
    report_to="none",
    
    # Hub
    push_to_hub=True,
    hub_model_id=REPO_ID,
    hub_strategy="checkpoint",
    
    # DDP
    ddp_find_unused_parameters=False,
    seed=42,
)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# === è‡ªå®šä¹‰å›è°ƒï¼šè®­ç»ƒæ—¶ç”Ÿæˆæ ·æœ¬æ–‡æœ¬ ===
from transformers import TrainerCallback

class GenerationCallback(TrainerCallback):
    """æ¯æ¬¡è¯„ä¼°æ—¶ç”Ÿæˆæ ·æœ¬æ–‡æœ¬ï¼Œç›´è§‚è§‚å¯Ÿæ¨¡å‹è¿›æ­¥"""
    
    def __init__(self, tokenizer, prompts=None):
        self.tokenizer = tokenizer
        self.prompts = prompts or ["ä¸­å›½çš„å†å²", "äººå·¥æ™ºèƒ½æ˜¯", "çŸ¥ä¹ä¸Šæœ‰äººé—®"]
    
    def on_evaluate(self, args, state, control, model, **kwargs):
        print("\n" + "=" * 50)
        print(f"ğŸ“ Step {state.global_step} - ç”Ÿæˆæ ·æœ¬:")
        print("=" * 50)
        
        model.eval()
        device = next(model.parameters()).device
        
        for prompt in self.prompts:
            try:
                inputs = self.tokenizer(prompt, return_tensors="pt").to(device)
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=50,
                        do_sample=True,
                        temperature=0.8,
                        top_k=50,
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id,
                    )
                generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                print(f"  [{prompt}] â†’ {generated}")
            except Exception as e:
                print(f"  [{prompt}] â†’ ç”Ÿæˆå¤±è´¥: {e}")
        
        print("=" * 50 + "\n")
        model.train()

# === è‡ªå®šä¹‰ Trainerï¼šä½¿ç”¨ Liger FusedLinearCrossEntropy ===
# æ˜¾å­˜å‡å°‘ 60-80%ï¼Œé€Ÿåº¦æå‡ 50-100%ï¼ˆå¤§è¯è¡¨æ•ˆæœæ›´æ˜æ˜¾ï¼‰
USE_FUSED_CE = True  # è®¾ç½®ä¸º False å¯ç¦ç”¨

try:
    from liger_kernel.transformers import LigerFusedLinearCrossEntropyLoss
    
    class LigerTrainer(Trainer):
        """ä½¿ç”¨ FusedLinearCrossEntropy çš„è‡ªå®šä¹‰ Trainer
        
        åŸç†ï¼šå°† LM Head (Linear) + CrossEntropy Loss èåˆä¸ºä¸€ä¸ª kernel
        ä¼˜åŠ¿ï¼š
        - ä¸éœ€è¦ materialize å®Œæ•´çš„ logits tensor (vocab_size Ã— batch Ã— seq)
        - åˆ†å—è®¡ç®—ï¼Œæ˜¾å­˜å ç”¨å¤§å¹…é™ä½
        - å‡å°‘å†…å­˜è®¿é—®ï¼ŒGPU åˆ©ç”¨ç‡æé«˜
        """
        
        def __init__(self, *args, lm_head_weight=None, **kwargs):
            super().__init__(*args, **kwargs)
            self.fused_loss_fn = LigerFusedLinearCrossEntropyLoss()
            self.lm_head_weight = lm_head_weight
            print("âœ… LigerTrainer: å·²å¯ç”¨ FusedLinearCrossEntropy")
        
        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
            """é‡å†™ loss è®¡ç®—ï¼Œä½¿ç”¨ Fused kernel"""
            labels = inputs.pop("labels")
            
            # è·å– hidden statesï¼ˆä¸ç»è¿‡ lm_headï¼‰
            # å¤„ç† torch.compile åŒ…è£…çš„æ¨¡å‹
            base_model = getattr(model, '_orig_mod', model)
            transformer = getattr(base_model, 'transformer', None)
            if transformer is None:
                # Fallback: ä½¿ç”¨æ ‡å‡† forward ç„¶åå¿½ç•¥ logits
                raise RuntimeError("æ— æ³•è®¿é—® model.transformerï¼Œè¯·ç¦ç”¨ USE_FUSED_CE")
            
            outputs = transformer(
                input_ids=inputs["input_ids"],
                attention_mask=inputs.get("attention_mask"),
            )
            hidden_states = outputs.last_hidden_state  # [batch, seq, hidden]
            
            # Shift: LM ä»»åŠ¡ä¸­ labels éœ€è¦å·¦ç§»ä¸€ä½
            # hidden_states[:-1] é¢„æµ‹ labels[1:]
            shift_hidden = hidden_states[..., :-1, :].contiguous()  # [batch, seq-1, hidden]
            shift_labels = labels[..., 1:].contiguous()  # [batch, seq-1]
            
            # Flatten for FusedLinearCrossEntropy
            batch_size, seq_len, hidden_size = shift_hidden.shape
            shift_hidden = shift_hidden.view(-1, hidden_size)  # [batch*seq-1, hidden]
            shift_labels = shift_labels.view(-1)  # [batch*seq-1]
            
            # ä½¿ç”¨ FusedLinearCrossEntropy
            # å®ƒæ¥å—: (weight, input, target)
            loss = self.fused_loss_fn(
                self.lm_head_weight,  # [vocab_size, hidden_size]
                shift_hidden,          # [batch*seq-1, hidden_size]
                shift_labels           # [batch*seq-1]
            )
            
            if return_outputs:
                # æ„é€ ä¸€ä¸ªå‡çš„ outputs å¯¹è±¡ç”¨äºå…¶ä»–å›è°ƒ
                from transformers.modeling_outputs import CausalLMOutputWithPast
                fake_outputs = CausalLMOutputWithPast(loss=loss)
                return loss, fake_outputs
            return loss
    
    if USE_FUSED_CE:
        TrainerClass = LigerTrainer
        trainer_kwargs = {"lm_head_weight": LM_HEAD_WEIGHT}  # ä½¿ç”¨é¢„ä¿å­˜çš„æƒé‡
        print("âœ… å°†ä½¿ç”¨ LigerTrainer + FusedLinearCrossEntropy")
    else:
        TrainerClass = Trainer
        trainer_kwargs = {}
        print("âš ï¸ FusedLinearCrossEntropy å·²ç¦ç”¨ï¼Œä½¿ç”¨æ ‡å‡† Trainer")
        
except ImportError:
    print("âš ï¸ liger-kernel æœªå®‰è£…ï¼Œä½¿ç”¨æ ‡å‡† Trainer")
    TrainerClass = Trainer
    trainer_kwargs = {}
except Exception as e:
    print(f"âš ï¸ LigerTrainer åˆå§‹åŒ–å¤±è´¥: {e}")
    print("   ä½¿ç”¨æ ‡å‡† Trainer ä½œä¸º fallback")
    TrainerClass = Trainer
    trainer_kwargs = {}

generation_callback = GenerationCallback(
    tokenizer=tokenizer,
    prompts=["ä¸­å›½çš„å†å²", "äººå·¥æ™ºèƒ½æ˜¯", "çŸ¥ä¹ä¸Šæœ‰äººé—®", "ç§‘å­¦æŠ€æœ¯çš„å‘å±•"]
)

trainer = TrainerClass(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    callbacks=[generation_callback],
    **trainer_kwargs,
)

print(f"âœ… è®­ç»ƒé…ç½®å®Œæˆ")
print(f"   é¢„ä¼°æ­¥æ•°: {total_steps}")
print(f"   Warmup: {training_args.warmup_steps}")
print(f"   âœ… å·²å¯ç”¨ç”Ÿæˆå›è°ƒï¼ˆæ¯ {training_args.eval_steps} æ­¥æµ‹è¯• promptï¼‰")
if TrainerClass.__name__ == "LigerTrainer":
    print(f"   âœ… FusedLinearCrossEntropy: æ˜¾å­˜ -60~80%, é€Ÿåº¦ +50~100%")

# ================== Cell 8: å¼€å§‹è®­ç»ƒ ==================
print("\n" + "=" * 60)
print("ğŸš€ ç¬¬ä¸ƒé˜¶æ®µï¼šå¼€å§‹è®­ç»ƒ")
print("=" * 60)

import time
print(f"â° å¼€å§‹: {time.strftime('%Y-%m-%d %H:%M:%S')}")
print(f"ğŸ“ æ¨¡å‹: https://huggingface.co/{REPO_ID}")
print("\nğŸ“ˆ Loss å‚è€ƒ: åˆå§‹ ~10, ç›®æ ‡ <3.5")
print("=" * 60)

try:
    result = trainer.train()
    print(f"\nâœ… å®Œæˆ! æœ€ç»ˆ Loss: {result.training_loss:.4f}")
except KeyboardInterrupt:
    print("\nâš ï¸ ä¸­æ–­ï¼Œä¿å­˜ä¸­...")
except Exception as e:
    print(f"\nâŒ é”™è¯¯: {e}")
finally:
    trainer.save_model()
    tokenizer.save_pretrained(training_args.output_dir)
    try:
        trainer.push_to_hub(commit_message="Training complete")
        print(f"âœ… å·²ä¸Šä¼ è‡³ HuggingFace")
    except:
        print("âš ï¸ ä¸Šä¼ å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨ä¸Šä¼ ")

print(f"\nğŸ‰ æ¨¡å‹: https://huggingface.co/{REPO_ID}")

# ================== Cell 9: ç”Ÿæˆæµ‹è¯• ==================
print("\n" + "=" * 60)
print("ğŸ”¬ ç”Ÿæˆæµ‹è¯•")
print("=" * 60)

model.eval()
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

prompts = ["ä¸­å›½çš„å†å²", "äººå·¥æ™ºèƒ½æ˜¯", "ç§‘å­¦æŠ€æœ¯", "çŸ¥ä¹ä¸Šæœ‰äººé—®"]

for prompt in prompts:
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs, max_length=80, do_sample=True,
            temperature=0.8, top_k=50, top_p=0.95,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    print(f"\n[{prompt}]")
    print(f"  â†’ {tokenizer.decode(outputs[0], skip_special_tokens=True)}")
