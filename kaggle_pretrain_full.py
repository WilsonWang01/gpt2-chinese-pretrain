# Kaggle Mini GPT-2 ä¸­æ–‡é¢„è®­ç»ƒ - å®Œæ•´ Notebook
# ===================================================
# å¤åˆ¶æ•´ä¸ªæ–‡ä»¶åˆ° Kaggle Notebook è¿è¡Œ
# è®¾ç½®: Accelerator = GPU T4 x2, Internet = On
# ===================================================

# ================== Cell 1: ç¯å¢ƒè®¾ç½® ==================
print("=" * 60)
print("ğŸ”§ ç¬¬ä¸€é˜¶æ®µï¼šç¯å¢ƒé…ç½®")
print("=" * 60)

# GPU æ£€æŸ¥
import subprocess
subprocess.run(["nvidia-smi"], check=True)

# å®‰è£…ä¾èµ–
subprocess.run([
    "pip", "install", "-q",
    "transformers==4.37.2",
    "datasets==2.16.1",
    "accelerate==0.26.1",
    "huggingface_hub==0.20.3",
    "sentencepiece==0.1.99",
    "tokenizers==0.15.1",
], check=True)

import torch
print(f"\nâœ… PyTorch: {torch.__version__}")
print(f"âœ… CUDA: {torch.cuda.is_available()}")
print(f"âœ… GPU æ•°é‡: {torch.cuda.device_count()}")

# ================== Cell 2: HuggingFace ç™»å½• ==================
from huggingface_hub import notebook_login, whoami

notebook_login()  # éœ€è¦è¾“å…¥ token

try:
    HF_USERNAME = whoami()["name"]
    print(f"âœ… ç™»å½•æˆåŠŸ: {HF_USERNAME}")
except:
    HF_USERNAME = "YOUR_USERNAME"  # æ‰‹åŠ¨å¡«å†™
    print(f"âš ï¸ è¯·æ‰‹åŠ¨è®¾ç½® HF_USERNAME")

# ================== Cell 3: æ•°æ®å‡†å¤‡ ==================
print("\n" + "=" * 60)
print("ğŸ“¥ ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®å‡†å¤‡")
print("=" * 60)

from datasets import load_dataset
import os

# åŠ è½½æ•°æ® (è°ƒæ•´æ¯”ä¾‹æ§åˆ¶æ•°æ®é‡)
DATA_RATIO = "10%"  # å¯é€‰: 10%, 50%, 100%
dataset = load_dataset(
    "pleisto/wikipedia-cn-20230720-filtered",
    split=f"train[:{DATA_RATIO}]",
    trust_remote_code=True
)
print(f"âœ… æ•°æ®æ ·æœ¬: {len(dataset)}")

# å¯¼å‡ºçº¯æ–‡æœ¬
CORPUS_FILE = "/kaggle/working/wiki_corpus.txt"
with open(CORPUS_FILE, "w", encoding="utf-8") as f:
    for item in dataset:
        text = item["completion"].strip()
        if len(text) > 50:
            f.write(text + "\n")

print(f"âœ… è¯­æ–™å¯¼å‡º: {os.path.getsize(CORPUS_FILE) / 1e6:.1f} MB")

# ================== Cell 4: åˆ†è¯å™¨è®­ç»ƒ ==================
print("\n" + "=" * 60)
print("ğŸ”¤ ç¬¬ä¸‰é˜¶æ®µï¼šåˆ†è¯å™¨è®­ç»ƒ")
print("=" * 60)

import sentencepiece as spm

MODEL_PREFIX = "/kaggle/working/chinese_sp"
VOCAB_SIZE = 32000

spm.SentencePieceTrainer.train(
    input=CORPUS_FILE,
    model_prefix=MODEL_PREFIX,
    vocab_size=VOCAB_SIZE,
    model_type="unigram",
    character_coverage=0.9995,
    pad_id=0, unk_id=1, bos_id=2, eos_id=3,
    pad_piece="<pad>", unk_piece="<unk>", 
    bos_piece="<s>", eos_piece="</s>",
    num_threads=os.cpu_count(),
    normalization_rule_name="nmt_nfkc_cf",
    split_by_unicode_script=True,
    split_by_number=True,
)
print(f"âœ… åˆ†è¯å™¨è®­ç»ƒå®Œæˆ")

# è½¬æ¢ä¸º HuggingFace æ ¼å¼
from transformers import LlamaTokenizerFast

TOKENIZER_DIR = "/kaggle/working/tokenizer"
tokenizer = LlamaTokenizerFast(
    vocab_file=f"{MODEL_PREFIX}.model",
    bos_token="<s>", eos_token="</s>",
    unk_token="<unk>", pad_token="<pad>",
    add_bos_token=False, add_eos_token=True,
)
tokenizer.save_pretrained(TOKENIZER_DIR)
print(f"âœ… è¯è¡¨å¤§å°: {len(tokenizer)}")

# æµ‹è¯•
test = "äººå·¥æ™ºèƒ½æ˜¯æœªæ¥çš„å‘å±•æ–¹å‘"
print(f"   æµ‹è¯•: {test}")
print(f"   åˆ†è¯: {tokenizer.tokenize(test)}")

# ================== Cell 5: æ¨¡å‹åˆå§‹åŒ– ==================
print("\n" + "=" * 60)
print("ğŸ§  ç¬¬å››é˜¶æ®µï¼šæ¨¡å‹åˆå§‹åŒ–")
print("=" * 60)

from transformers import GPT2Config, GPT2LMHeadModel, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)

config = GPT2Config(
    vocab_size=len(tokenizer),
    n_positions=512, n_ctx=512,
    n_embd=768, n_layer=6, n_head=12, n_inner=3072,
    activation_function="gelu_new",
    resid_pdrop=0.1, embd_pdrop=0.1, attn_pdrop=0.1,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id,
)

model = GPT2LMHeadModel(config)
print(f"âœ… å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M")

# ================== Cell 6: æ•°æ®å¤„ç† ==================
print("\n" + "=" * 60)
print("ğŸ“¦ ç¬¬äº”é˜¶æ®µï¼šæ•°æ®å¤„ç†")
print("=" * 60)

BLOCK_SIZE = 512

def tokenize_function(examples):
    return tokenizer(examples["completion"], truncation=False, return_attention_mask=False)

tokenized = dataset.map(
    tokenize_function, batched=True, batch_size=1000,
    remove_columns=dataset.column_names, num_proc=4
)

def group_texts(examples):
    concatenated = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = (len(concatenated["input_ids"]) // BLOCK_SIZE) * BLOCK_SIZE
    result = {k: [t[i:i+BLOCK_SIZE] for i in range(0, total_length, BLOCK_SIZE)] for k, t in concatenated.items()}
    result["labels"] = result["input_ids"].copy()
    return result

lm_dataset = tokenized.map(group_texts, batched=True, batch_size=1000, num_proc=4)
print(f"âœ… è®­ç»ƒæ ·æœ¬: {len(lm_dataset)} ä¸ª 512-token å—")
print(f"   æ€» Token: {len(lm_dataset) * BLOCK_SIZE / 1e6:.1f}M")

# ================== Cell 7: è®­ç»ƒé…ç½® ==================
print("\n" + "=" * 60)
print("âš™ï¸ ç¬¬å…­é˜¶æ®µï¼šè®­ç»ƒé…ç½®")
print("=" * 60)

from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling

MODEL_NAME = "gpt2-chinese-mini"
REPO_ID = f"{HF_USERNAME}/{MODEL_NAME}"

NUM_EPOCHS = 10
BATCH_SIZE = 16
GRAD_ACCUM = 2

total_steps = (len(lm_dataset) // (BATCH_SIZE * GRAD_ACCUM * 2)) * NUM_EPOCHS

training_args = TrainingArguments(
    output_dir=f"/kaggle/working/{MODEL_NAME}",
    overwrite_output_dir=True,
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRAD_ACCUM,
    
    # å­¦ä¹ ç‡è°ƒåº¦
    learning_rate=6e-4,
    warmup_steps=min(500, total_steps // 10),
    lr_scheduler_type="cosine",
    
    # ä¼˜åŒ–å™¨
    optim="adamw_torch",
    weight_decay=0.1,
    adam_beta1=0.9, adam_beta2=0.95,
    max_grad_norm=1.0,
    
    # ç²¾åº¦
    fp16=True,
    dataloader_num_workers=4,
    
    # Checkpoint
    save_strategy="steps",
    save_steps=200,
    save_total_limit=3,
    load_best_model_at_end=True,
    
    # æ—¥å¿—
    logging_steps=50,
    logging_first_step=True,
    report_to="none",
    
    # Hub
    push_to_hub=True,
    hub_model_id=REPO_ID,
    hub_strategy="checkpoint",
    
    # DDP
    ddp_find_unused_parameters=False,
    seed=42,
)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

print(f"âœ… è®­ç»ƒé…ç½®å®Œæˆ")
print(f"   é¢„ä¼°æ­¥æ•°: {total_steps}")
print(f"   Warmup: {training_args.warmup_steps}")

# ================== Cell 8: å¼€å§‹è®­ç»ƒ ==================
print("\n" + "=" * 60)
print("ğŸš€ ç¬¬ä¸ƒé˜¶æ®µï¼šå¼€å§‹è®­ç»ƒ")
print("=" * 60)

import time
print(f"â° å¼€å§‹: {time.strftime('%Y-%m-%d %H:%M:%S')}")
print(f"ğŸ“ æ¨¡å‹: https://huggingface.co/{REPO_ID}")
print("\nğŸ“ˆ Loss å‚è€ƒ: åˆå§‹ ~10, ç›®æ ‡ <4.0")
print("=" * 60)

try:
    result = trainer.train()
    print(f"\nâœ… å®Œæˆ! æœ€ç»ˆ Loss: {result.training_loss:.4f}")
except KeyboardInterrupt:
    print("\nâš ï¸ ä¸­æ–­ï¼Œä¿å­˜ä¸­...")
except Exception as e:
    print(f"\nâŒ é”™è¯¯: {e}")
finally:
    trainer.save_model()
    tokenizer.save_pretrained(training_args.output_dir)
    try:
        trainer.push_to_hub(commit_message="Training checkpoint")
        print(f"âœ… å·²ä¸Šä¼ è‡³ HuggingFace")
    except:
        print("âš ï¸ ä¸Šä¼ å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨ä¸Šä¼ ")

print(f"\nğŸ‰ æ¨¡å‹: https://huggingface.co/{REPO_ID}")

# ================== Cell 9: ç”Ÿæˆæµ‹è¯• ==================
print("\n" + "=" * 60)
print("ğŸ”¬ ç”Ÿæˆæµ‹è¯•")
print("=" * 60)

model.eval()
prompts = ["ä¸­å›½çš„å†å²", "äººå·¥æ™ºèƒ½æ˜¯", "ç§‘å­¦æŠ€æœ¯"]

for prompt in prompts:
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    with torch.no_grad():
        outputs = model.generate(
            **inputs, max_length=80, do_sample=True,
            temperature=0.8, top_k=50, top_p=0.95,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    print(f"\næç¤º: {prompt}")
    print(f"ç”Ÿæˆ: {tokenizer.decode(outputs[0], skip_special_tokens=True)}")
